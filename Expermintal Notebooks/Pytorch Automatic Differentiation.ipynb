{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd package in PyTorch\n",
    "\n",
    "The goal of this notebook is to play around with this package to get a better understanding of how PyTorch does automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "We need to wrap our tensors as variables to be able to generate the back propagation and our gradients.\n",
    "\n",
    "Here, we create a simple scaler value $x=4$.  Even though it is nondimensional, we still need to make it a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([4]), requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single variable polynomial\n",
    "\n",
    "## $z = x^{3} + 2x^{2} + 8x$\n",
    "\n",
    "We will use a lambda function for this polynominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_1 = lambda x: torch.pow(x, 3) + 2*torch.pow(x, 2) + 8*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Derivative\n",
    "\n",
    "Let's manually do the math to get the first derivative and evaluate it for $x=4$.\n",
    "\n",
    "### $\\frac{dz}{dx} = 3x^{2} + 4x + 8$\n",
    "\n",
    "### $\\frac{dz}{dx}\\Bigr|_{\\substack{x=4}} \\quad 3*4^{2} + 4*4 + 8 = 72$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see if PyTorch gets the same results\n",
    "\n",
    "Generate a new tensor $z$ that is the result of our function.\n",
    "\n",
    "We then take this result tensor and run the back propagation method.\n",
    "\n",
    "This sets the input variable $x$ grad attribute to first derivative evaluated at $x=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([72.])\n"
     ]
    }
   ],
   "source": [
    "z = func_1(x)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph\n",
    "\n",
    "Let's write our polynomial as composite functions to show how it would look as a computational graph.\n",
    "\n",
    "We will also add the derivative with respect to the local variable $w_i$\n",
    "\n",
    "## $z = x^{3} + 2x^{2} + 8x$\n",
    "\n",
    "$w_1 = x \\quad \\rightarrow \\quad \\dot{w_1} = 1$\n",
    "\n",
    "$w_2 = {w_1}^{3} \\quad \\rightarrow \\quad \\dot{w_2} = 3{w_1}^2$\n",
    "\n",
    "$w_3 = {w_1}^{2} \\quad \\rightarrow \\quad \\dot{w_3} = 2{w_1}$\n",
    "\n",
    "$w_4 = 2{w_3} \\quad \\rightarrow \\quad \\dot{w_4} = 2\\dot{w_3} \\quad$ *(chain rule)*\n",
    "\n",
    "$w_5 = 8{w_2} \\quad \\rightarrow \\quad \\dot{w_5} = 8$\n",
    "\n",
    "$w_6 = w_2 + w_4 + w_3 \\quad \\rightarrow \\quad \\dot{w_6} = \\dot{w_2} + \\dot{w_4} + \\dot{w_3}$\n",
    "\n",
    "\n",
    "Now let's find the numerical derivative for each node using $x=4$:\n",
    "\n",
    "$\\dot{w_2} = 3x^2 = 48$\n",
    "\n",
    "$\\dot{w_3} = 2x = 8$\n",
    "\n",
    "$\\dot{w_4} = 2\\dot{w_3} = 16$\n",
    "\n",
    "$\\dot{w_5} = 8$\n",
    "\n",
    "$\\dot{w_6} = \\dot{w_2} + \\dot{w_4} + \\dot{w_5} = 48 + 16 + 8 = 72$\n",
    "\n",
    "#### This is how the gradient for the forward propagation is stored.  \n",
    "\n",
    "PyTorch does not use calculus per se, it has a set of known derivatives for most math functions.\n",
    "\n",
    "During the forward, the gradient is stored as a numeric value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When backward is run, it is actually doing matrix multiplication\n",
    "\n",
    "The gradient is stored as a matrix (\"Jacobian\").  This is not a real Jacobian with symbolic derivatives, but an array of the values of the gradient like we did above with our computation graph.  It is a matrix since there can be more than 1  variable and multiple rows of values. You can think of each row of values as a vector which makes our function a vector valued function.\n",
    "\n",
    "To get the values, you pass in a vector and this vector is multiplied by the Jacobian matrix (Jacobian vector product).\n",
    "\n",
    "This looks odd since you create a vector of ones.  The ones vector means it will return the values of the gradients unchanged.  You can weight the vector to scale the output, but since we want to see the raw gradients, we use a vector of ones.\n",
    "\n",
    "*Note: If there is only 1 row and a single variable (scalar), we can use default parameter for the backward function and not build the vector of ones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([72.])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad and find z again\n",
    "x.grad.data.zero_()\n",
    "z = func_1(x) \n",
    "\n",
    "vector_of_ones = torch.ones(x.shape[0])\n",
    "z.backward(vector_of_ones)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The computational graph is release automatically.  So running backward() twice throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    z.backward()\n",
    "except RuntimeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we tell PyTorch to ratain the computation graph, we can run backward() multiple times\n",
    "\n",
    "But all this does is add sum the gradients again.\n",
    "\n",
    "72\n",
    "\n",
    "72+72=144\n",
    "\n",
    "72+72+72=216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([72.])\n",
      "tensor([144.])\n",
      "tensor([216.])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad and find z again\n",
    "x.grad.data.zero_()\n",
    "z = func_1(x)\n",
    "\n",
    "z.backward(retain_graph = True)\n",
    "print(x.grad)\n",
    "z.backward(retain_graph = True)\n",
    "print(x.grad)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a different function that is not a polynominal\n",
    "\n",
    "### $z = sin(x)$\n",
    "\n",
    "$\\frac {dz}{dx} = cos(x)$\n",
    "\n",
    "$\\frac{dz}{dx}\\Bigr|_{\\substack{x=4}} \\quad cos(4) \\approx  -0.6536$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6536])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad and find z again\n",
    "x.grad.data.zero_()\n",
    "z = func_1(x) \n",
    "\n",
    "func_2 = lambda x: torch.sin(x)\n",
    "z = func_2(x)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple values, but still with a single variable\n",
    "\n",
    "We will use the same function above, $z = sin(x)$\n",
    "\n",
    "But now instead of passing in just one value for x, we will pass in an array of 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.6419, 1.1369, 4.7996, 0.8205, 0.4615, 0.4596, 2.5004, 1.2695, 2.5845,\n",
       "        0.4513], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Variable(2*np.pi * torch.rand(([10])), requires_grad=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We are now required to manually build and pass in the vector of ones since we no longer have a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\n",
      " tensor([-0.5982,  0.9073, -0.9962,  0.7315,  0.4453,  0.4436,  0.5981,  0.9549,\n",
      "         0.5287,  0.4362], grad_fn=<SinBackward>)\n",
      "\n",
      "PyTorch Gradient:\n",
      " [ 0.80134296  0.420412    0.08707973  0.6818824   0.8953653   0.896241\n",
      " -0.80138785  0.29677498 -0.84881455  0.89987385]\n",
      "\n",
      "Validate with cosine:\n",
      " [ 0.80134296  0.420412    0.08707973  0.6818824   0.8953653   0.896241\n",
      " -0.80138785  0.29677498 -0.84881455  0.89987385]\n"
     ]
    }
   ],
   "source": [
    "Z = func_2(X)\n",
    "print(f'Z:\\n {Z}')\n",
    "vector_of_ones = torch.ones(X.shape)\n",
    "Z.backward(vector_of_ones)\n",
    "print(f'\\nPyTorch Gradient:\\n {X.grad.data.numpy()}')\n",
    "#since we know Z' = cosine, we can varifiy the results from autograd\n",
    "print(f'\\nValidate with cosine:\\n {torch.cos(X).data.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 scaler variables\n",
    "\n",
    "We already defined $x=4$.\n",
    "\n",
    "Now let's define $y=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# reset the grad on x\n",
    "x.grad.data.zero_()\n",
    "\n",
    "y = Variable(torch.FloatTensor([2]), requires_grad=True)\n",
    "print(y)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynominal with 2 variables\n",
    "\n",
    "### $z = x^{3} + y^{2} + ax + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_3 = lambda x, y, a, b: torch.pow(x, 3) + torch.pow(y, 2) + a*x + b\n",
    "z = func_3(x, y, 1, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial z}{\\partial x} = 3x^{2} + a$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}\\Bigr|_{\\substack{x=4, a=1}} \\quad 3*4^{2} + 1 = 49$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y} = 2y$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y}\\Bigr|_{\\substack{y=2}} \\quad 2*2 = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49.])\n",
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sine function with 2 scalar variables\n",
    "\n",
    "### $z = sin(x) + sin(y)$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}\\Bigr|_{\\substack{x=4}} \\quad cos(4) \\approx  -0.6536$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y}\\Bigr|_{\\substack{y=2}} \\quad cos(2) \\approx  -0.4161$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6536])\n",
      "tensor([-0.4161])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad on x and y\n",
    "x.grad.data.zero_()\n",
    "y.grad.data.zero_()\n",
    "\n",
    "func_4 = lambda x, y: torch.sin(x) + torch.sin(y)\n",
    "z = func_4(x, y)\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use 2 variables, each with multiple values\n",
    "\n",
    "We will build a matrix (ndarray) with 10 rows and 2 columns.\n",
    "\n",
    "The first column will be the X values and the second column will be the Y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.9066, 1.1849],\n",
       "        [6.0019, 1.2083],\n",
       "        [3.8134, 4.7397],\n",
       "        [2.1871, 2.9670],\n",
       "        [0.4878, 4.3015],\n",
       "        [2.5796, 3.2423],\n",
       "        [1.0124, 5.7254],\n",
       "        [4.4697, 4.1561],\n",
       "        [5.1514, 6.0768],\n",
       "        [0.3598, 1.5586]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY = Variable(2*np.pi * torch.rand(([10, 2])), requires_grad=True)\n",
    "XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5587,  0.6574, -1.6221,  0.9898, -0.4481,  0.4324,  0.3188, -1.8199,\n",
       "        -1.1101,  1.3520], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_5 = lambda XY: torch.sin(XY[:,0]) + torch.sin(XY[:,1])\n",
    "Z = func_5(XY)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradiant will also have 2 columns\n",
    "\n",
    "The first column is the X gradients and the second column is the Y gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Gradients:\n",
      " [[ 0.9299176   0.37643185]\n",
      " [ 0.9606921   0.3545953 ]\n",
      " [-0.78267014  0.02728887]\n",
      " [-0.578051   -0.9847898 ]\n",
      " [ 0.88338107 -0.39942288]\n",
      " [-0.8461944  -0.9949382 ]\n",
      " [ 0.5298115   0.8484107 ]\n",
      " [-0.24032214 -0.5280292 ]\n",
      " [ 0.42501867  0.9787835 ]\n",
      " [ 0.9359573   0.01222623]]\n",
      "\n",
      "Validate with cosine:\n",
      " [[ 0.9299176   0.37643185]\n",
      " [ 0.9606921   0.3545953 ]\n",
      " [-0.78267014  0.02728887]\n",
      " [-0.578051   -0.9847898 ]\n",
      " [ 0.88338107 -0.39942288]\n",
      " [-0.8461944  -0.9949382 ]\n",
      " [ 0.5298115   0.8484107 ]\n",
      " [-0.24032214 -0.5280292 ]\n",
      " [ 0.42501867  0.9787835 ]\n",
      " [ 0.9359573   0.01222623]]\n"
     ]
    }
   ],
   "source": [
    "vector_of_ones = torch.ones(XY.shape[0]) # We just want this to be a vector, so we just use the row count (shape[0])\n",
    "Z.backward(vector_of_ones)\n",
    "\n",
    "print(f'\\nPyTorch Gradients:\\n {XY.grad.data.numpy()}')\n",
    "#since we know Z' = cosine, we can varifiy the results from autograd\n",
    "print(f'\\nValidate with cosine:\\n {torch.cos(XY).data.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "### $z = tanh(x)$\n",
    "\n",
    "\n",
    "### $\\frac{dz}{dx} = 1 - tanh^2(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1015,  0.4655,  0.7771, -0.6037, -0.8693,  0.2803, -0.9955, -0.4249,\n",
       "         0.3446, -0.9768], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Variable(2 * torch.rand(([10])) - 1, requires_grad=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1012,  0.4345,  0.6510, -0.5397, -0.7010,  0.2732, -0.7597, -0.4010,\n",
       "         0.3316, -0.7517], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_6 = lambda X: torch.tanh(X)\n",
    "Z = func_6(X)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Gradients:\n",
      " [0.9897621  0.8111834  0.57615125 0.7087302  0.5085927  0.9253647\n",
      " 0.42288297 0.8391874  0.8900712  0.43500286]\n",
      "\n",
      "Validate with 1-tanh^2:\n",
      " [0.9897621  0.8111834  0.57615125 0.7087302  0.5085927  0.9253647\n",
      " 0.42288297 0.8391874  0.8900712  0.43500286]\n"
     ]
    }
   ],
   "source": [
    "vector_of_ones = torch.ones(X.shape) \n",
    "Z.backward(vector_of_ones)\n",
    "\n",
    "print(f'\\nPyTorch Gradients:\\n {X.grad.data.numpy()}')\n",
    "#since we know Z' = 1-tanh^2, we can varifiy the results from autograd\n",
    "print(f'\\nValidate with 1-tanh^2:\\n {(1 - torch.pow(torch.tanh(X), 2)).data.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
