{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd package in PyTorch\n",
    "\n",
    "The goal of this notebook is to play around with this package to get a better understanding of how PyTorch does automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "We need to wrap our tensors as variables to be able to generate the back propagation and our gradients.\n",
    "\n",
    "Here, we create a simple scaler value $x=4$.  Even though it is nondimensional, we still need to make it a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([4]), requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single variable polynomial\n",
    "\n",
    "## $z = x^{3} + 2x^{2} + 8x$\n",
    "\n",
    "We will use a lambda function for this polynominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_1 = lambda x: torch.pow(x, 3) + 2*torch.pow(x, 2) + 8*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Derivative\n",
    "\n",
    "Let's manually do the math to get the first derivative and evaluate it for $x=4$.\n",
    "\n",
    "### $\\frac{dz}{dx} = 3x^{2} + 4x + 8$\n",
    "\n",
    "### $\\frac{dz}{dx}\\Bigr|_{\\substack{x=4}} \\quad 3*4^{2} + 4*4 + 8 = 72$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see if PyTorch gets the same results\n",
    "\n",
    "Generate a new tensor $z$ that is the result of our function.\n",
    "\n",
    "We then take this result tensor and run the back propagation method.\n",
    "\n",
    "This sets the input variable $x$ grad attribute to first derivative evaluated at $x=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([72.])\n"
     ]
    }
   ],
   "source": [
    "z = func_1(x)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph\n",
    "\n",
    "Let's write our polynomial as composite functions to show how it would look as a computational graph.\n",
    "\n",
    "We will also add the derivative with respect to the local variable $w_i$\n",
    "\n",
    "## $z = x^{3} + 2x^{2} + 8x$\n",
    "\n",
    "$w_1 = x \\quad \\rightarrow \\quad \\dot{w_1} = 1$\n",
    "\n",
    "$w_2 = {w_1}^{3} \\quad \\rightarrow \\quad \\dot{w_2} = 3{w_1}^2$\n",
    "\n",
    "$w_3 = {w_1}^{2} \\quad \\rightarrow \\quad \\dot{w_3} = 2{w_1}$\n",
    "\n",
    "$w_4 = 2{w_3} \\quad \\rightarrow \\quad \\dot{w_4} = 2\\dot{w_3} \\quad$ *(chain rule)*\n",
    "\n",
    "$w_5 = 8{w_2} \\quad \\rightarrow \\quad \\dot{w_5} = 8$\n",
    "\n",
    "$w_6 = w_2 + w_4 + w_3 \\quad \\rightarrow \\quad \\dot{w_6} = \\dot{w_2} + \\dot{w_4} + \\dot{w_3}$\n",
    "\n",
    "\n",
    "#### Now let's find the numerical derivative for each node using $x=4$:\n",
    "\n",
    "${w_1} = x = 4$\n",
    "\n",
    "$\\dot{w_2} = 3{w_1}^2 = 48$\n",
    "\n",
    "$\\dot{w_3} = 2{w_1} = 8$\n",
    "\n",
    "$\\dot{w_4} = 2\\dot{w_3} = 16$\n",
    "\n",
    "$\\dot{w_5} = 8$\n",
    "\n",
    "$\\dot{w_6} = \\dot{w_2} + \\dot{w_4} + \\dot{w_5} = 48 + 16 + 8 = 72$\n",
    "\n",
    "#### This is how the gradient for the forward propagation is stored.  \n",
    "\n",
    "PyTorch does not use calculus per se, it has a set of known derivative equations for most math functions.\n",
    "\n",
    "During the forward, the gradient is stored as a numeric value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When backward is run, it is actually doing matrix multiplication\n",
    "\n",
    "The gradient is stored as a matrix (\"Jacobian\").  This is not the real Jacobian with symbolic derivatives, but an array of the values of the gradients like we did above with our computation graph.  It is a matrix since there can be multiple independant and dependant varialbes (features and loss functions). \n",
    "\n",
    "Why would their be multiple loss functions?  Think multi-class classification.  Let's say we have 3 lables or targets: Cat, Dog, and Deer.  Each one of these targets would have their own loss function.  We then combine these 5 losses into a single value.  So in this case, the Jacobian would have 3 rows. You can think of each row of values as the graidents for that loss function.\n",
    "\n",
    "i.e. The gradient for loss funciton $y_1$ with 3 features is:\n",
    "\n",
    "$\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2}, \\frac{\\partial y_1}{\\partial x_3}$\n",
    "\n",
    "The gradient for loss funciton $y_2$ is:\n",
    "\n",
    "$\\frac{\\partial y_2}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2}, \\frac{\\partial y_2}{\\partial x_3}$\n",
    "\n",
    "Symbolically, the Jacobian looks like this for 3 features [$x_1, x_2, x_3$] with 3 loss functions [$y_1, y_2, y_3$]\n",
    "\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "  \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} \\\\ \n",
    "  \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_3}\\\\ \n",
    "  \\frac{\\partial y_3}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_3}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Since on the forward pass, PyTorch only stored the numeric derivatives, the \"Jacobian\" would looke more like:\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "  0.145 & 24.6 & 1.57 \\\\ \n",
    "  135 & 0.457 & 45.24\\\\ \n",
    " .001 & -75.4 & -22\n",
    "\\end{pmatrix}$\n",
    "\n",
    "In our first example $z = x^{3} + 2x^{2} + 8x$, we only had one function and one independant variable. so the Jacobian values would simply be for $x=4$:\n",
    "\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "  72\n",
    "\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses this matrix of derivative values in matrix multiplication, called Jacobian vector product (JVP).  \n",
    "\n",
    "$ J \\cdot v  = \\begin{pmatrix}\n",
    "  \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} \\\\ \n",
    "  \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_3}\\\\ \n",
    "  \\frac{\\partial y_3}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_3}\n",
    "\\end{pmatrix}\\begin{pmatrix}\\frac{\\partial l}{\\partial y_1} \\\\ \\frac{\\partial l}{\\partial y_2} \\\\ \\frac{\\partial l}{\\partial y_3} \\end{pmatrix}$\n",
    "\n",
    "Looking at this with numeric values instead of symbolically we get something that looks like:\n",
    "\n",
    "$ J \\cdot v  = \\begin{pmatrix}\n",
    "  0.145 & 24.6 & 1.57 \\\\ \n",
    "  135 & 0.457 & 45.24\\\\ \n",
    " .001 & -75.4 & -22\n",
    "\\end{pmatrix}\\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "But what is this vector we are multiplying with the Jacobian?  \n",
    "\n",
    "This looks odd since you create a vector of ones.  The ones vector means it will return the values of the gradients unchanged.  You can weight the vector to scale the output, but since we want to see the raw gradients, we use a vector of ones.\n",
    "\n",
    "*Note: If there is only 1 row and a single variable (scalar), we can use default parameter for the backward function and not build the vector of ones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([72.])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad and find z again\n",
    "x.grad.data.zero_()\n",
    "z = func_1(x) \n",
    "\n",
    "vector_of_ones = torch.ones(x.shape[0])\n",
    "z.backward(vector_of_ones)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The computational graph is release automatically.  So running backward() twice throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    z.backward()\n",
    "except RuntimeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we tell PyTorch to ratain the computation graph, we can run backward() multiple times\n",
    "\n",
    "But all this does is add sum the gradients again.\n",
    "\n",
    "72\n",
    "\n",
    "72+72=144\n",
    "\n",
    "72+72+72=216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([72.])\n",
      "tensor([144.])\n",
      "tensor([216.])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad and find z again\n",
    "x.grad.data.zero_()\n",
    "z = func_1(x)\n",
    "\n",
    "z.backward(retain_graph = True)\n",
    "print(x.grad)\n",
    "z.backward(retain_graph = True)\n",
    "print(x.grad)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a different function that is not a polynominal\n",
    "\n",
    "### $z = sin(x)$\n",
    "\n",
    "$\\frac {dz}{dx} = cos(x)$\n",
    "\n",
    "$\\frac{dz}{dx}\\Bigr|_{\\substack{x=4}} \\quad cos(4) \\approx  -0.6536$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6536])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad and find z again\n",
    "x.grad.data.zero_()\n",
    "z = func_1(x) \n",
    "\n",
    "func_2 = lambda x: torch.sin(x)\n",
    "z = func_2(x)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple values, but still with a single variable\n",
    "\n",
    "We will use the same function above, $z = sin(x)$\n",
    "\n",
    "But now instead of passing in just one value for x, we will pass in an array of 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7813, 3.7756, 3.2924, 0.4196, 1.4990, 0.4501, 5.0463, 3.4687, 3.1123,\n",
       "        0.6195], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Variable(2*np.pi * torch.rand(([10])), requires_grad=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We are now required to manually build and pass in the vector of ones since we no longer have a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\n",
      " tensor([-0.9976, -0.5924, -0.1502,  0.4074,  0.9974,  0.4350, -0.9448, -0.3213,\n",
      "         0.0293,  0.5806], grad_fn=<SinBackward>)\n",
      "\n",
      "PyTorch Gradient:\n",
      " [ 0.06890509 -0.8056764  -0.9886568   0.913239    0.0717485   0.90042096\n",
      "  0.3277364  -0.94696265 -0.99957055  0.81416917]\n",
      "\n",
      "Validate with cosine:\n",
      " [ 0.06890509 -0.8056764  -0.9886568   0.913239    0.0717485   0.90042096\n",
      "  0.3277364  -0.94696265 -0.99957055  0.81416917]\n"
     ]
    }
   ],
   "source": [
    "Z = func_2(X)\n",
    "print(f'Z:\\n {Z}')\n",
    "vector_of_ones = torch.ones(X.shape)\n",
    "Z.backward(vector_of_ones)\n",
    "print(f'\\nPyTorch Gradient:\\n {X.grad.data.numpy()}')\n",
    "#since we know Z' = cosine, we can varifiy the results from autograd\n",
    "print(f'\\nValidate with cosine:\\n {torch.cos(X).data.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 scaler variables\n",
    "\n",
    "We already defined $x=4$.\n",
    "\n",
    "Now let's define $y=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# reset the grad on x\n",
    "x.grad.data.zero_()\n",
    "\n",
    "y = Variable(torch.FloatTensor([2]), requires_grad=True)\n",
    "print(y)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynominal with 2 variables\n",
    "\n",
    "### $z = x^{3} + y^{2} + ax + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_3 = lambda x, y, a, b: torch.pow(x, 3) + torch.pow(y, 2) + a*x + b\n",
    "z = func_3(x, y, 1, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial z}{\\partial x} = 3x^{2} + a$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}\\Bigr|_{\\substack{x=4, a=1}} \\quad 3*4^{2} + 1 = 49$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y} = 2y$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y}\\Bigr|_{\\substack{y=2}} \\quad 2*2 = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49.])\n",
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sine function with 2 scalar variables\n",
    "\n",
    "### $z = sin(x) + sin(y)$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}\\Bigr|_{\\substack{x=4}} \\quad cos(4) \\approx  -0.6536$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y}\\Bigr|_{\\substack{y=2}} \\quad cos(2) \\approx  -0.4161$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6536])\n",
      "tensor([-0.4161])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad on x and y\n",
    "x.grad.data.zero_()\n",
    "y.grad.data.zero_()\n",
    "\n",
    "func_4 = lambda x, y: torch.sin(x) + torch.sin(y)\n",
    "z = func_4(x, y)\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use 2 variables, each with multiple values\n",
    "\n",
    "We will build a matrix (ndarray) with 10 rows and 2 columns.\n",
    "\n",
    "The first column will be the X values and the second column will be the Y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0094, 6.0213],\n",
       "        [5.6559, 6.0421],\n",
       "        [2.0618, 5.9421],\n",
       "        [5.0784, 3.5384],\n",
       "        [0.3907, 5.4411],\n",
       "        [4.1734, 1.2095],\n",
       "        [3.5784, 1.2620],\n",
       "        [1.0616, 0.6195],\n",
       "        [5.2747, 4.1935],\n",
       "        [4.7013, 0.1220]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY = Variable(2*np.pi * torch.rand(([10, 2])), requires_grad=True)\n",
    "XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1271, -0.8257,  0.5473, -1.3202, -0.3652,  0.0772,  0.5296,  1.4538,\n",
       "        -1.7144, -0.8783], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_5 = lambda XY: torch.sin(XY[:,0]) + torch.sin(XY[:,1])\n",
    "Z = func_5(XY)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradiant will also have 2 columns\n",
    "\n",
    "The first column is the X gradients and the second column is the Y gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Gradients:\n",
      " [[-0.9912731   0.96590924]\n",
      " [ 0.80962276  0.9710875 ]\n",
      " [-0.47149777  0.9423794 ]\n",
      " [ 0.35788697 -0.9222973 ]\n",
      " [ 0.9246604   0.6659272 ]\n",
      " [-0.5132384   0.35351163]\n",
      " [-0.9060897   0.30387682]\n",
      " [ 0.48744553  0.8141897 ]\n",
      " [ 0.5331006  -0.49592763]\n",
      " [-0.01111674  0.9925727 ]]\n",
      "\n",
      "Validate with cosine:\n",
      " [[-0.9912731   0.96590924]\n",
      " [ 0.80962276  0.9710875 ]\n",
      " [-0.47149777  0.9423794 ]\n",
      " [ 0.35788697 -0.9222973 ]\n",
      " [ 0.9246604   0.6659272 ]\n",
      " [-0.5132384   0.35351163]\n",
      " [-0.9060897   0.30387682]\n",
      " [ 0.48744553  0.8141897 ]\n",
      " [ 0.5331006  -0.49592763]\n",
      " [-0.01111674  0.9925727 ]]\n"
     ]
    }
   ],
   "source": [
    "vector_of_ones = torch.ones(XY.shape[0]) # We just want this to be a vector, so we just use the row count (shape[0])\n",
    "Z.backward(vector_of_ones)\n",
    "\n",
    "print(f'\\nPyTorch Gradients:\\n {XY.grad.data.numpy()}')\n",
    "#since we know Z' = cosine, we can varifiy the results from autograd\n",
    "print(f'\\nValidate with cosine:\\n {torch.cos(XY).data.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "### $z = \\tanh(x)$\n",
    "\n",
    "\n",
    "### $\\frac{dz}{dx} = 1 - \\tanh^2(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9306,  0.0117,  0.8363, -0.9083, -0.9654, -0.8753, -0.6202,  0.4318,\n",
       "        -0.8084,  0.2868], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Variable(2 * torch.rand(([10])) - 1, requires_grad=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7309,  0.0117,  0.6838, -0.7203, -0.7467, -0.7041, -0.5513,  0.4068,\n",
       "        -0.6687,  0.2792], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_6 = lambda X: torch.tanh(X)\n",
    "Z = func_6(X)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Gradients:\n",
      " [0.4657911  0.99986243 0.5323725  0.4811691  0.4424963  0.5042944\n",
      " 0.696074   0.8345099  0.55286425 0.92203015]\n",
      "\n",
      "Validate with 1-tanh^2:\n",
      " [0.4657911  0.99986243 0.5323725  0.4811691  0.4424963  0.5042944\n",
      " 0.696074   0.8345099  0.55286425 0.92203015]\n"
     ]
    }
   ],
   "source": [
    "vector_of_ones = torch.ones(X.shape) \n",
    "Z.backward(vector_of_ones)\n",
    "\n",
    "print(f'\\nPyTorch Gradients:\\n {X.grad.data.numpy()}')\n",
    "#since we know Z' = 1-tanh^2, we can varifiy the results from autograd\n",
    "print(f'\\nValidate with 1-tanh^2:\\n {(1 - torch.pow(torch.tanh(X), 2)).data.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression and Loss Function\n",
    "\n",
    "We want to find a way to take our gradients and apply them in a way simular to what a NN would use.\n",
    "\n",
    "Lets define a single variable higher order polynominal function:\n",
    "\n",
    "### $z = 6.2x^6 + 0.14x^5 + 721x^3 + 0.002x^2 + 4x + 124$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_actual = lambda x:  \\\n",
    "                    6.2*torch.pow(x, 6) + \\\n",
    "                   0.14*torch.pow(x, 5) + \\\n",
    "                      0*torch.pow(x, 4) + \\\n",
    "                    721*torch.pow(x, 3) + \\\n",
    "                  0.002*torch.pow(x, 2) + \\\n",
    "                      4*x               + \\\n",
    "                    124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define another function, but without the hard coded coefficients:\n",
    "\n",
    "### $\\hat{z} = {w_{10}}x^{10} + {w_9}x^9 + {w_8}x^8 + {w_7}x^7 + {w_6}x^6 + {w_5}x^5 + {w_4}x^4 + {w_3}x^3 + {w_2}x^2 + {w_1}x + {w_0}$\n",
    "\n",
    "This second function will be what we use for our predictions.  \n",
    "\n",
    "#### The goal is to train the prediction function to actual function by adjusting the weights we pass in\n",
    "\n",
    "We will start out all the weights = 1:\n",
    "\n",
    "$\\hat{z} = {1}x^{10} + {1}x^9 + {1}x^8 + {1}x^7 + {1}x^6 + {1}x^5 + {1}x^4 + {1}x^3 + {1}x^2 + {1}x + {1}$\n",
    "\n",
    "We then build a loss function that finds the difference between actual and predicted.\n",
    "\n",
    "Next, we take the loss function and call backward().  This should give us the gradients for 11 weight values.\n",
    "\n",
    "Then we adjust each of the weights to increment by some negative fraction of the weight's gradient.\n",
    "\n",
    "We repeat this process in a loop until we train the weight coefficients to match as close as possible the actual coefficients in our actual function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_predict = lambda x, W: \\\n",
    "                   W[10]*torch.pow(x, 10) + \\\n",
    "                    W[9]*torch.pow(x, 9) + \\\n",
    "                    W[8]*torch.pow(x, 8) + \\\n",
    "                    W[7]*torch.pow(x, 7) + \\\n",
    "                    W[6]*torch.pow(x, 6) + \\\n",
    "                    W[5]*torch.pow(x, 5) + \\\n",
    "                    W[4]*torch.pow(x, 4) + \\\n",
    "                    W[3]*torch.pow(x, 3) + \\\n",
    "                    W[2]*torch.pow(x, 2) + \\\n",
    "                    W[1]*x               + \\\n",
    "                    W[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start out by visualizing the actual function along with the predicting functions with 1 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_samples = torch.arange(-50, 35, 1) / 10.\n",
    "z_values =  torch.squeeze(torch.tensor([func_actual(x) for x in x_samples]))\n",
    "\n",
    "f = plt.figure(figsize=(8, 4))\n",
    "plt.plot(x_samples, z_values, color='b')\n",
    "plt.hlines(y=0, xmin=-5, xmax=3, color='black')\n",
    "plt.vlines(x=0, ymin=torch.squeeze(z_values).min(), ymax=torch.squeeze(z_values).max(), color='black')\n",
    "plt.title('Actual Polynominal')\n",
    "plt.show()\n",
    "\n",
    "one_weights = torch.ones([11])\n",
    "z_hat_one_values = torch.tensor([func_predict(x, one_weights) for x in x_samples])\n",
    "\n",
    "f = plt.figure(figsize=(8, 4))\n",
    "plt.plot(x_samples, torch.squeeze(z_hat_one_values), color='r')\n",
    "plt.hlines(y=0, xmin=-5, xmax=3, color='black')\n",
    "plt.vlines(x=0, ymin=torch.squeeze(z_hat_one_values).min(), ymax=torch.squeeze(z_hat_one_values).max(), color='black')\n",
    "plt.title('Predicted Polynominal with One Weights')\n",
    "plt.show()\n",
    "\n",
    "known_weights = torch.tensor([124, 4, 0.002, 721, 0, 0.14, 6.2, 0, 0, 0, 0])\n",
    "z_hat_known_values = torch.tensor([func_predict(x, known_weights) for x in x_samples])\n",
    "\n",
    "f = plt.figure(figsize=(8, 4))\n",
    "plt.plot(x_samples, torch.squeeze(z_hat_known_values), color='g')\n",
    "plt.hlines(y=0, xmin=-5, xmax=3, color='black')\n",
    "plt.vlines(x=0, ymin=torch.squeeze(z_hat_known_values).min(), ymax=torch.squeeze(z_hat_known_values).max(), color='black')\n",
    "plt.title('Predicted Polynominal with Known Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see, the first 2 curves are very different.\n",
    "#### But if we input the right weights into our prediction function as we did in the 3rd curve, we get an exact fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap Everything as a Variable\n",
    "\n",
    "We need to make sure that all every item in the computational graph for the loss function is a Variable.\n",
    "\n",
    "This include the actual and predicted values, the samples of x we will use and the weights.\n",
    "\n",
    "If we miss wrapping any item in the graph, we will get an error when we call loss.backward()\n",
    "\n",
    "#### There are several ways to generate a tensor with caculates a gradiant on the forward pass. \n",
    "\n",
    "We will use the torch.autograd.Variable approach in most of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youci\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad:\n",
      " ones: False         \n",
      " ones_deprecated: True         \n",
      " ones_tensor: True         \n",
      " ones_variable: True\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones([3])\n",
    "ones_deprecated  = torch.ones([3], requires_grad=True)\n",
    "ones_tensor = torch.tensor(torch.ones([3]), requires_grad=True)\n",
    "ones_variable = Variable((torch.ones([3])), requires_grad=True)\n",
    "\n",
    "print(f'requires_grad:\\n ones: {ones.requires_grad} \\\n",
    "        \\n ones_deprecated: {ones_deprecated.requires_grad} \\\n",
    "        \\n ones_tensor: {ones_tensor.requires_grad} \\\n",
    "        \\n ones_variable: {ones_variable.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = Variable(torch.arange(-500, 500, 1) / 10., requires_grad=True) # add more samples than we used in our plots\n",
    "z_values =  Variable(torch.squeeze(torch.tensor([func_actual(x) for x in x_samples])), requires_grad=True)\n",
    "\n",
    "\n",
    "one_weights = Variable(torch.ones([11]), requires_grad=True)\n",
    "z_ones = func_predict(x_samples, one_weights)\n",
    "\n",
    "known_weights = Variable(torch.tensor([124, 4, 0.002, 721, 0, 0.14, 6.2, 0, 0, 0, 0]), requires_grad=True)\n",
    "z_known = func_predict(x_samples, known_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Now let's try to define a loss function to quantify the differences between the 2 results\n",
    "\n",
    "We will use Mean Squared Error (MSE) for our loss function.  This will use an $L_2$ distance and therefore penalize outliers more.\n",
    "\n",
    "The loss function doesn't have to be normalized or averaged, we just need some quantitative way to access the degree of inaccuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunction(Y, Y_pred):\n",
    "    return torch.mean((Y - Y_pred)) / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with 1 weights: -8,880,541,138,944.0\n",
      "Loss with known weights: 0.0\n"
     ]
    }
   ],
   "source": [
    "ones_loss = lossFunction(z_values, z_ones)\n",
    "known_loss = lossFunction(z_values, z_known)\n",
    "print(f'Loss with 1 weights: {ones_loss:,}\\nLoss with known weights: {known_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We get a very large loss with all the weights set to one, but our loss is Zero if we get the exact weights right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make sure all inputs and our 2 loss functions are ready for backprop.\n",
    "\n",
    "*Note: The loss function will only show the last gradient function which should be based on torch.sum*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_samples: True\n",
      "z_values: True\n",
      "one_weights: True\n",
      "z_hat_one_values: False\n",
      "known_weights: True\n",
      "z_hat_known_values: False\n",
      "ones_loss: <DivBackward0 object at 0x000001E4E7037548>\n",
      "known_loss: <DivBackward0 object at 0x000001E4E7037548>\n"
     ]
    }
   ],
   "source": [
    "print(f'x_samples: {x_samples.requires_grad}\\n\\\n",
    "z_values: {z_values.requires_grad}\\n\\\n",
    "one_weights: {one_weights.requires_grad}\\n\\\n",
    "z_hat_one_values: {z_hat_one_values.requires_grad}\\n\\\n",
    "known_weights: {known_weights.requires_grad}\\n\\\n",
    "z_hat_known_values: {z_hat_known_values.requires_grad}\\n\\\n",
    "ones_loss: {ones_loss.grad_fn}\\n\\\n",
    "known_loss: {known_loss.grad_fn}'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackProp\n",
    "\n",
    "We will run the backward() on our 2 loss results.  Since the loss function produces a scalar value, we do not need to pass in the vector of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for the Known Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One Weight Gradients:\n",
      " [-1.00000028e-03  4.99998278e-05 -8.33334923e-01  1.24999955e-01\n",
      " -1.25000830e+03  3.12499817e+02 -2.23217400e+06  7.81250688e+05\n",
      " -4.34038170e+09  1.95312486e+09 -8.87816611e+12]\n"
     ]
    }
   ],
   "source": [
    "known_loss.backward()\n",
    "print(f'\\nOne Weight Gradients:\\n {known_weights.grad.data.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All 11 gradient values are zero since the known weights produce an exact match**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for the Weights with all values = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One Weight Gradients:\n",
      " [-1.00000028e-03  4.99998278e-05 -8.33334923e-01  1.24999955e-01\n",
      " -1.25000830e+03  3.12499817e+02 -2.23217400e+06  7.81250688e+05\n",
      " -4.34038170e+09  1.95312486e+09 -8.87816611e+12]\n"
     ]
    }
   ],
   "source": [
    "ones_loss.backward()\n",
    "print(f'\\nOne Weight Gradients:\\n {one_weights.grad.data.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The 11 gradient values corrlate to the 11 weights**\n",
    "\n",
    "$\\dot {w_0} \\approx 2e+19 \\quad $ This shows the contribution of the *y-intercept* to the loss\n",
    "\n",
    "$\\dot {w_6} \\approx 2e+29 \\quad $ This shows the contribution of the $x^6$ cofficent's to the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFeatures(value, degree=2):\n",
    "    result = []\n",
    "    for i in range(degree):\n",
    "        result.append(value**i)\n",
    "    return result\n",
    "def ployFeaturePoints(start, end, stride, degree=2):\n",
    "    result = []\n",
    "    points = np.arange(start, end, stride)\n",
    "    np.random.shuffle(points)\n",
    "    for i in points:\n",
    "        result.append(polyFeatures(i, degree))\n",
    "    return torch.tensor(result)\n",
    "def zeroGrad(t):\n",
    "    if t.grad is not None:\n",
    "        t.grad.data.zero_()\n",
    "def zeroAllGrads():\n",
    "    zeroGrad(x)\n",
    "    zeroGrad(z)\n",
    "    zeroGrad(z_pred)\n",
    "    zeroGrad(training_weights)\n",
    "def printWeights(W):\n",
    "                    print(f'\\nWeights: \\n\\\n",
    "10:{W[10]:,.2f} \\n\\\n",
    " 9:{W[9]:,.2f}  \\n\\\n",
    " 8:{W[8]:,.2f}  \\n\\\n",
    " 7:{W[7]:,.2f}  \\n\\\n",
    " 6:{W[6]:,.2f}  \\n\\\n",
    " 5:{W[5]:,.2f}  \\n\\\n",
    " 4:{W[4]:,.2f}  \\n\\\n",
    " 3:{W[3]:,.2f}  \\n\\\n",
    " 2:{W[2]:,.2f}  \\n\\\n",
    " 1:{W[1]:,.2f}  \\n\\\n",
    " 0:{W[0]:,.2f}  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights: \n",
      "10:1.00 \n",
      " 9:1.00  \n",
      " 8:1.00  \n",
      " 7:1.00  \n",
      " 6:1.00  \n",
      " 5:1.00  \n",
      " 4:1.00  \n",
      " 3:1.00  \n",
      " 2:1.00  \n",
      " 1:1.00  \n",
      " 0:1.00  \n"
     ]
    }
   ],
   "source": [
    "x = Variable(ployFeaturePoints(-50, 50, .1, 10), requires_grad=True)\n",
    "z = Variable(torch.squeeze(torch.tensor([func_actual(i[1]) for i in x])), requires_grad=True)\n",
    "\n",
    "criterion = lossFunction\n",
    "lr = 1e-2\n",
    "\n",
    "training_weights = Variable(torch.ones([11]), requires_grad=True)\n",
    "printWeights(training_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.4031e+22, grad_fn=<MulBackward0>)\n",
      "tensor(3.7477e+22, grad_fn=<MulBackward0>)\n",
      "tensor(5.1878e+24, grad_fn=<MulBackward0>)\n",
      "tensor(3.9199e+24, grad_fn=<MulBackward0>)\n",
      "tensor(8.4059e+24, grad_fn=<MulBackward0>)\n",
      "tensor(4.4337e+20, grad_fn=<MulBackward0>)\n",
      "tensor(9.8491e+17, grad_fn=<MulBackward0>)\n",
      "tensor(5.6216e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.6451e+12, grad_fn=<MulBackward0>)\n",
      "tensor(2.7107e+24, grad_fn=<MulBackward0>)\n",
      "tensor(2.1396e+09, grad_fn=<MulBackward0>)\n",
      "tensor(6.8490e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.2033e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.2393e+18, grad_fn=<MulBackward0>)\n",
      "tensor(9.4805e+12, grad_fn=<MulBackward0>)\n",
      "tensor(1.4669e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.4278e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.9247e+22, grad_fn=<MulBackward0>)\n",
      "tensor(2.7020e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.9299e+27, grad_fn=<MulBackward0>)\n",
      "tensor(-2.6848e+10, grad_fn=<MulBackward0>)\n",
      "tensor(8.5822e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.4725e+18, grad_fn=<MulBackward0>)\n",
      "tensor(7.1110e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.7137e+18, grad_fn=<MulBackward0>)\n",
      "tensor(4.3552e+27, grad_fn=<MulBackward0>)\n",
      "tensor(98353.1953, grad_fn=<MulBackward0>)\n",
      "tensor(-7.7985e+09, grad_fn=<MulBackward0>)\n",
      "tensor(5.5746e+19, grad_fn=<MulBackward0>)\n",
      "tensor(3.1628e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.1067e+21, grad_fn=<MulBackward0>)\n",
      "tensor(7.3892e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.3799e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.1782e+24, grad_fn=<MulBackward0>)\n",
      "tensor(7.1527e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.4096e+20, grad_fn=<MulBackward0>)\n",
      "tensor(6.4338e+21, grad_fn=<MulBackward0>)\n",
      "tensor(3.8058e+24, grad_fn=<MulBackward0>)\n",
      "tensor(4.5795e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.2096e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.4044e+26, grad_fn=<MulBackward0>)\n",
      "tensor(7.4842e+26, grad_fn=<MulBackward0>)\n",
      "tensor(-148.8716, grad_fn=<MulBackward0>)\n",
      "tensor(5.0037e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.6422e+10, grad_fn=<MulBackward0>)\n",
      "tensor(4.3267e+23, grad_fn=<MulBackward0>)\n",
      "tensor(5.6323e+19, grad_fn=<MulBackward0>)\n",
      "tensor(-12539902., grad_fn=<MulBackward0>)\n",
      "tensor(1.0443e+20, grad_fn=<MulBackward0>)\n",
      "tensor(8.4815e+18, grad_fn=<MulBackward0>)\n",
      "tensor(4.2226e+24, grad_fn=<MulBackward0>)\n",
      "tensor(3.3549e+20, grad_fn=<MulBackward0>)\n",
      "tensor(2.0341e+26, grad_fn=<MulBackward0>)\n",
      "tensor(6.5895e+16, grad_fn=<MulBackward0>)\n",
      "tensor(2.3082e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.2757e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.4955e+09, grad_fn=<MulBackward0>)\n",
      "tensor(3.0146e+24, grad_fn=<MulBackward0>)\n",
      "tensor(2.4757e+14, grad_fn=<MulBackward0>)\n",
      "tensor(2.4952e+16, grad_fn=<MulBackward0>)\n",
      "tensor(2.8589e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.4755e+22, grad_fn=<MulBackward0>)\n",
      "tensor(8.8989e+15, grad_fn=<MulBackward0>)\n",
      "tensor(2.8329e+23, grad_fn=<MulBackward0>)\n",
      "tensor(5.3251e+20, grad_fn=<MulBackward0>)\n",
      "tensor(3.1946e+18, grad_fn=<MulBackward0>)\n",
      "tensor(8.5745e+26, grad_fn=<MulBackward0>)\n",
      "tensor(5.7495e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.4204e+17, grad_fn=<MulBackward0>)\n",
      "tensor(9.7690e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.9377e+22, grad_fn=<MulBackward0>)\n",
      "tensor(7.5512e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.1467e+21, grad_fn=<MulBackward0>)\n",
      "tensor(7.4783e+14, grad_fn=<MulBackward0>)\n",
      "tensor(1.5644e+23, grad_fn=<MulBackward0>)\n",
      "tensor(7.8500e+17, grad_fn=<MulBackward0>)\n",
      "tensor(9.1690e+14, grad_fn=<MulBackward0>)\n",
      "tensor(9.4979e+11, grad_fn=<MulBackward0>)\n",
      "tensor(9.0530e+12, grad_fn=<MulBackward0>)\n",
      "tensor(1.2799e+12, grad_fn=<MulBackward0>)\n",
      "tensor(5.6674e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.6799e+18, grad_fn=<MulBackward0>)\n",
      "tensor(1.3706e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.4470e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.3324e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.3982e+27, grad_fn=<MulBackward0>)\n",
      "tensor(8.7213e+23, grad_fn=<MulBackward0>)\n",
      "tensor(5.8537e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.1946e+15, grad_fn=<MulBackward0>)\n",
      "tensor(6.2455e+22, grad_fn=<MulBackward0>)\n",
      "tensor(-3.5743e+10, grad_fn=<MulBackward0>)\n",
      "tensor(1.2580e+21, grad_fn=<MulBackward0>)\n",
      "tensor(8.9824e+17, grad_fn=<MulBackward0>)\n",
      "tensor(1.3341e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.4111e+24, grad_fn=<MulBackward0>)\n",
      "tensor(2.2061e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.1275e+18, grad_fn=<MulBackward0>)\n",
      "tensor(4.9787e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.9818e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.3339e+14, grad_fn=<MulBackward0>)\n",
      "tensor(3.8932e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.8980e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.6519e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.2373e+24, grad_fn=<MulBackward0>)\n",
      "tensor(4.0203e+12, grad_fn=<MulBackward0>)\n",
      "tensor(5.2307e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1224.8159, grad_fn=<MulBackward0>)\n",
      "tensor(3.6062e+20, grad_fn=<MulBackward0>)\n",
      "tensor(3.6116e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.0716e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.0046e+16, grad_fn=<MulBackward0>)\n",
      "tensor(2.0463e+20, grad_fn=<MulBackward0>)\n",
      "tensor(6.6285e+25, grad_fn=<MulBackward0>)\n",
      "tensor(6.4374e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.8154e+27, grad_fn=<MulBackward0>)\n",
      "tensor(5.3420e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.3315e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.0169e+22, grad_fn=<MulBackward0>)\n",
      "tensor(7.2310e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.2098e+23, grad_fn=<MulBackward0>)\n",
      "tensor(111.7082, grad_fn=<MulBackward0>)\n",
      "tensor(1.2947e+13, grad_fn=<MulBackward0>)\n",
      "tensor(1.6681e+21, grad_fn=<MulBackward0>)\n",
      "tensor(3.4954e+22, grad_fn=<MulBackward0>)\n",
      "tensor(2.2127e+23, grad_fn=<MulBackward0>)\n",
      "tensor(4.3488e+24, grad_fn=<MulBackward0>)\n",
      "tensor(3.4736e+12, grad_fn=<MulBackward0>)\n",
      "tensor(3.2402e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.4836e+27, grad_fn=<MulBackward0>)\n",
      "tensor(8.8332e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.6671e+13, grad_fn=<MulBackward0>)\n",
      "tensor(3.7217e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.7789e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.0181e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.5161e+19, grad_fn=<MulBackward0>)\n",
      "tensor(3.2379e+13, grad_fn=<MulBackward0>)\n",
      "tensor(2.3545e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.6140e+25, grad_fn=<MulBackward0>)\n",
      "tensor(5.3191e+18, grad_fn=<MulBackward0>)\n",
      "tensor(2.5182e+11, grad_fn=<MulBackward0>)\n",
      "tensor(7.7057e+16, grad_fn=<MulBackward0>)\n",
      "tensor(9.6051e+18, grad_fn=<MulBackward0>)\n",
      "tensor(8.6890e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.3718e+26, grad_fn=<MulBackward0>)\n",
      "tensor(9.1125e+20, grad_fn=<MulBackward0>)\n",
      "tensor(41480424., grad_fn=<MulBackward0>)\n",
      "tensor(9.0174e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.0655e+26, grad_fn=<MulBackward0>)\n",
      "tensor(7.9122e+22, grad_fn=<MulBackward0>)\n",
      "tensor(6.8527e+17, grad_fn=<MulBackward0>)\n",
      "tensor(1.0451e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.0865e+14, grad_fn=<MulBackward0>)\n",
      "tensor(1.1722e+18, grad_fn=<MulBackward0>)\n",
      "tensor(1.9042e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.2092e+16, grad_fn=<MulBackward0>)\n",
      "tensor(4.8599e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.7027e+21, grad_fn=<MulBackward0>)\n",
      "tensor(4.9828e+10, grad_fn=<MulBackward0>)\n",
      "tensor(1.6141e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.2192e+27, grad_fn=<MulBackward0>)\n",
      "tensor(8.9697e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.0615e+16, grad_fn=<MulBackward0>)\n",
      "tensor(7.5316e+18, grad_fn=<MulBackward0>)\n",
      "tensor(5.6853e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.9378e+14, grad_fn=<MulBackward0>)\n",
      "tensor(9.0141e+21, grad_fn=<MulBackward0>)\n",
      "tensor(7.6338e+20, grad_fn=<MulBackward0>)\n",
      "tensor(5.9423e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.4067e+21, grad_fn=<MulBackward0>)\n",
      "tensor(4.1275e+21, grad_fn=<MulBackward0>)\n",
      "tensor(5.2386e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.1856e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.3770e+25, grad_fn=<MulBackward0>)\n",
      "tensor(5.4596e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.9745e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.8152e+21, grad_fn=<MulBackward0>)\n",
      "tensor(2.7577e+17, grad_fn=<MulBackward0>)\n",
      "tensor(1.9529e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.7420e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.5500e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.6707e+14, grad_fn=<MulBackward0>)\n",
      "tensor(-1.7435e+10, grad_fn=<MulBackward0>)\n",
      "tensor(2.8307e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.9171e+15, grad_fn=<MulBackward0>)\n",
      "tensor(5.0694e+19, grad_fn=<MulBackward0>)\n",
      "tensor(5.5627e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.7673e+21, grad_fn=<MulBackward0>)\n",
      "tensor(3.3379e+25, grad_fn=<MulBackward0>)\n",
      "tensor(-2.0978e+09, grad_fn=<MulBackward0>)\n",
      "tensor(1.8582e+24, grad_fn=<MulBackward0>)\n",
      "tensor(3.2476e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.7749e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.7334e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.3522e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.5336e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.3764e+23, grad_fn=<MulBackward0>)\n",
      "tensor(3.9364e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.3915e+24, grad_fn=<MulBackward0>)\n",
      "tensor(4.1331e+13, grad_fn=<MulBackward0>)\n",
      "tensor(2.7273e+15, grad_fn=<MulBackward0>)\n",
      "tensor(1.0316e+27, grad_fn=<MulBackward0>)\n",
      "tensor(-31365.3457, grad_fn=<MulBackward0>)\n",
      "tensor(1.0755e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.8075e+26, grad_fn=<MulBackward0>)\n",
      "tensor(7.1595e+21, grad_fn=<MulBackward0>)\n",
      "tensor(6.0436e+24, grad_fn=<MulBackward0>)\n",
      "tensor(2.1793e+18, grad_fn=<MulBackward0>)\n",
      "tensor(703.5305, grad_fn=<MulBackward0>)\n",
      "tensor(4.9683e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.7271e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.7274e+23, grad_fn=<MulBackward0>)\n",
      "tensor(4.2220e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.0074e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.8846e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.3497e+13, grad_fn=<MulBackward0>)\n",
      "tensor(1.4780e+22, grad_fn=<MulBackward0>)\n",
      "tensor(4.0507e+15, grad_fn=<MulBackward0>)\n",
      "tensor(5.7446e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.3823e+20, grad_fn=<MulBackward0>)\n",
      "tensor(2.4276e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.3370e+18, grad_fn=<MulBackward0>)\n",
      "tensor(6.3571e+24, grad_fn=<MulBackward0>)\n",
      "tensor(2.0240e+15, grad_fn=<MulBackward0>)\n",
      "tensor(1.0267e+18, grad_fn=<MulBackward0>)\n",
      "tensor(1.0030e+26, grad_fn=<MulBackward0>)\n",
      "tensor(7.3842e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.5907e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.5260e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.9506e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.3125e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.2953e+20, grad_fn=<MulBackward0>)\n",
      "tensor(3.6837e+20, grad_fn=<MulBackward0>)\n",
      "tensor(-185976.2188, grad_fn=<MulBackward0>)\n",
      "tensor(2.4643e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.7377e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.4402e+22, grad_fn=<MulBackward0>)\n",
      "tensor(7.6087e+24, grad_fn=<MulBackward0>)\n",
      "tensor(9.7094e+26, grad_fn=<MulBackward0>)\n",
      "tensor(5.3217e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.5115e+17, grad_fn=<MulBackward0>)\n",
      "tensor(2.6304e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.7213e+25, grad_fn=<MulBackward0>)\n",
      "tensor(5.4843e+23, grad_fn=<MulBackward0>)\n",
      "tensor(7.9585e+26, grad_fn=<MulBackward0>)\n",
      "tensor(5.5291e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.7346e+27, grad_fn=<MulBackward0>)\n",
      "tensor(5.6659e+17, grad_fn=<MulBackward0>)\n",
      "tensor(4.4987e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.0892e+21, grad_fn=<MulBackward0>)\n",
      "tensor(4.3043e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.6244e+21, grad_fn=<MulBackward0>)\n",
      "tensor(8.7496e+21, grad_fn=<MulBackward0>)\n",
      "tensor(3.7206e+24, grad_fn=<MulBackward0>)\n",
      "tensor(3.6215e+18, grad_fn=<MulBackward0>)\n",
      "tensor(1.5559e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.0193e+26, grad_fn=<MulBackward0>)\n",
      "tensor(5.7442e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.4270e+13, grad_fn=<MulBackward0>)\n",
      "tensor(2.0688e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.7115e+22, grad_fn=<MulBackward0>)\n",
      "tensor(7.0545e+11, grad_fn=<MulBackward0>)\n",
      "tensor(4.3441e+21, grad_fn=<MulBackward0>)\n",
      "tensor(6.9423e+26, grad_fn=<MulBackward0>)\n",
      "tensor(7.6379e+22, grad_fn=<MulBackward0>)\n",
      "tensor(5.2580e+13, grad_fn=<MulBackward0>)\n",
      "tensor(1.7761e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.2898e+12, grad_fn=<MulBackward0>)\n",
      "tensor(8.4529e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.2919e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.1282e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.5297e+20, grad_fn=<MulBackward0>)\n",
      "tensor(3.8503e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.9098e+25, grad_fn=<MulBackward0>)\n",
      "tensor(9.2808e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.3470e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.3509e+21, grad_fn=<MulBackward0>)\n",
      "tensor(72057456., grad_fn=<MulBackward0>)\n",
      "tensor(2.7195e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.5162e+23, grad_fn=<MulBackward0>)\n",
      "tensor(4.0725e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.6865e+20, grad_fn=<MulBackward0>)\n",
      "tensor(2.1225e+19, grad_fn=<MulBackward0>)\n",
      "tensor(2.1938e+22, grad_fn=<MulBackward0>)\n",
      "tensor(2.6546e+19, grad_fn=<MulBackward0>)\n",
      "tensor(4.2650e+17, grad_fn=<MulBackward0>)\n",
      "tensor(2.7277e+11, grad_fn=<MulBackward0>)\n",
      "tensor(1.1549e+20, grad_fn=<MulBackward0>)\n",
      "tensor(4.1189e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.8918e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.2192e+22, grad_fn=<MulBackward0>)\n",
      "tensor(5.9753e+17, grad_fn=<MulBackward0>)\n",
      "tensor(6.1138e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.0789e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.5233e+18, grad_fn=<MulBackward0>)\n",
      "tensor(3.0399e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.5235e+19, grad_fn=<MulBackward0>)\n",
      "tensor(6.3593e+23, grad_fn=<MulBackward0>)\n",
      "tensor(-4.0034e+10, grad_fn=<MulBackward0>)\n",
      "tensor(7.4263e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.3556e+19, grad_fn=<MulBackward0>)\n",
      "tensor(2.4772e+20, grad_fn=<MulBackward0>)\n",
      "tensor(3.1890e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.7961e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.1358e+14, grad_fn=<MulBackward0>)\n",
      "tensor(8.2359e+23, grad_fn=<MulBackward0>)\n",
      "tensor(7.7338e+21, grad_fn=<MulBackward0>)\n",
      "tensor(6.5952e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.6403e+18, grad_fn=<MulBackward0>)\n",
      "tensor(4.6390e+09, grad_fn=<MulBackward0>)\n",
      "tensor(54.4882, grad_fn=<MulBackward0>)\n",
      "tensor(5.3210e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.0856e+21, grad_fn=<MulBackward0>)\n",
      "tensor(2.1447e+23, grad_fn=<MulBackward0>)\n",
      "tensor(4.1850e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.3891e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.5182e+19, grad_fn=<MulBackward0>)\n",
      "tensor(6.0733e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.1577e+24, grad_fn=<MulBackward0>)\n",
      "tensor(5.8124e+25, grad_fn=<MulBackward0>)\n",
      "tensor(5.5003e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.5025e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.3697e+19, grad_fn=<MulBackward0>)\n",
      "tensor(5.0506e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.1428e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.9233e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.5020e+16, grad_fn=<MulBackward0>)\n",
      "tensor(-2.9852e+09, grad_fn=<MulBackward0>)\n",
      "tensor(2.6642e+23, grad_fn=<MulBackward0>)\n",
      "tensor(8.6860e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.9462e+14, grad_fn=<MulBackward0>)\n",
      "tensor(1.4312e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.8403e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.0293e+20, grad_fn=<MulBackward0>)\n",
      "tensor(7.5692e+23, grad_fn=<MulBackward0>)\n",
      "tensor(-4.3212e+08, grad_fn=<MulBackward0>)\n",
      "tensor(7.0002e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.3615e+22, grad_fn=<MulBackward0>)\n",
      "tensor(8.2611e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.6100e+27, grad_fn=<MulBackward0>)\n",
      "tensor(-594.9380, grad_fn=<MulBackward0>)\n",
      "tensor(1.5573e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.5924e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.4676e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.4782e+18, grad_fn=<MulBackward0>)\n",
      "tensor(1.8997e+19, grad_fn=<MulBackward0>)\n",
      "tensor(3.5139e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.1375e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.3014e+11, grad_fn=<MulBackward0>)\n",
      "tensor(4.5624e+26, grad_fn=<MulBackward0>)\n",
      "tensor(5.3815e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.5841e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.0618e+14, grad_fn=<MulBackward0>)\n",
      "tensor(6.9275e+23, grad_fn=<MulBackward0>)\n",
      "tensor(-4247.6919, grad_fn=<MulBackward0>)\n",
      "tensor(3.4014e+23, grad_fn=<MulBackward0>)\n",
      "tensor(7.4709e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.0464e+25, grad_fn=<MulBackward0>)\n",
      "tensor(7.1443e+22, grad_fn=<MulBackward0>)\n",
      "tensor(4.9283e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.3133e+15, grad_fn=<MulBackward0>)\n",
      "tensor(4.7972e+16, grad_fn=<MulBackward0>)\n",
      "tensor(2.3916e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.8409e+22, grad_fn=<MulBackward0>)\n",
      "tensor(7.7698e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.8091e+21, grad_fn=<MulBackward0>)\n",
      "tensor(4.5772e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.1245e+27, grad_fn=<MulBackward0>)\n",
      "tensor(929741.1875, grad_fn=<MulBackward0>)\n",
      "tensor(-4.1793e+09, grad_fn=<MulBackward0>)\n",
      "tensor(4.4546e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.2859e+09, grad_fn=<MulBackward0>)\n",
      "tensor(1.7819e+16, grad_fn=<MulBackward0>)\n",
      "tensor(4.0112e+21, grad_fn=<MulBackward0>)\n",
      "tensor(5.0889e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.9623e+24, grad_fn=<MulBackward0>)\n",
      "tensor(5.9671e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.2473e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.9339e+17, grad_fn=<MulBackward0>)\n",
      "tensor(1.2906e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.9921e+20, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4139e+12, grad_fn=<MulBackward0>)\n",
      "tensor(6.5175e+25, grad_fn=<MulBackward0>)\n",
      "tensor(7.0829e+13, grad_fn=<MulBackward0>)\n",
      "tensor(1.6159e+24, grad_fn=<MulBackward0>)\n",
      "tensor(4.8734e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.7484e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.9966e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.3020e+23, grad_fn=<MulBackward0>)\n",
      "tensor(-1.4501e+09, grad_fn=<MulBackward0>)\n",
      "tensor(3.1332e+22, grad_fn=<MulBackward0>)\n",
      "tensor(7.3403e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.6654e+24, grad_fn=<MulBackward0>)\n",
      "tensor(2.9262e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.6399e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.0977e+22, grad_fn=<MulBackward0>)\n",
      "tensor(2.5324e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.3486e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.2278e+08, grad_fn=<MulBackward0>)\n",
      "tensor(1.9138e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.4898e+26, grad_fn=<MulBackward0>)\n",
      "tensor(7.6663e+26, grad_fn=<MulBackward0>)\n",
      "tensor(6.5179e+17, grad_fn=<MulBackward0>)\n",
      "tensor(2.1184e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.6438e+19, grad_fn=<MulBackward0>)\n",
      "tensor(1.2216e+17, grad_fn=<MulBackward0>)\n",
      "tensor(1.6989e+19, grad_fn=<MulBackward0>)\n",
      "tensor(3.1068e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.5312e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.3007e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.5968e+19, grad_fn=<MulBackward0>)\n",
      "tensor(5.0264e+23, grad_fn=<MulBackward0>)\n",
      "tensor(7.4886e+17, grad_fn=<MulBackward0>)\n",
      "tensor(2.5619e+17, grad_fn=<MulBackward0>)\n",
      "tensor(5.3216e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.8499e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.6255e+21, grad_fn=<MulBackward0>)\n",
      "tensor(-277.9712, grad_fn=<MulBackward0>)\n",
      "tensor(1.0917e+26, grad_fn=<MulBackward0>)\n",
      "tensor(5.0215e+19, grad_fn=<MulBackward0>)\n",
      "tensor(5.2450e+18, grad_fn=<MulBackward0>)\n",
      "tensor(3.6553e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.6919e+18, grad_fn=<MulBackward0>)\n",
      "tensor(2.9646e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.1572e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.5087e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.4104e+11, grad_fn=<MulBackward0>)\n",
      "tensor(1.0841e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.2622e+24, grad_fn=<MulBackward0>)\n",
      "tensor(5.4536e+22, grad_fn=<MulBackward0>)\n",
      "tensor(2.2286e+15, grad_fn=<MulBackward0>)\n",
      "tensor(2.7782e+20, grad_fn=<MulBackward0>)\n",
      "tensor(2.8304e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.8808e+18, grad_fn=<MulBackward0>)\n",
      "tensor(1.0975e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.1268e+19, grad_fn=<MulBackward0>)\n",
      "tensor(1.9527e+23, grad_fn=<MulBackward0>)\n",
      "tensor(3.4708e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.5242e+13, grad_fn=<MulBackward0>)\n",
      "tensor(1.2388e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.6602e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.9714e+18, grad_fn=<MulBackward0>)\n",
      "tensor(124., grad_fn=<MulBackward0>)\n",
      "tensor(1.7739e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.2271e+14, grad_fn=<MulBackward0>)\n",
      "tensor(1.5292e+24, grad_fn=<MulBackward0>)\n",
      "tensor(5.8160e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.2494e+16, grad_fn=<MulBackward0>)\n",
      "tensor(2.9453e+16, grad_fn=<MulBackward0>)\n",
      "tensor(5.3325e+23, grad_fn=<MulBackward0>)\n",
      "tensor(4.8706e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.9195e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.2909e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.7161e+12, grad_fn=<MulBackward0>)\n",
      "tensor(2.2860e+22, grad_fn=<MulBackward0>)\n",
      "tensor(3.3032e+19, grad_fn=<MulBackward0>)\n",
      "tensor(6.8566e+19, grad_fn=<MulBackward0>)\n",
      "tensor(4.6108e+22, grad_fn=<MulBackward0>)\n",
      "tensor(3.5596e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.4384e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.0981e+19, grad_fn=<MulBackward0>)\n",
      "tensor(3.5812e+15, grad_fn=<MulBackward0>)\n",
      "tensor(2.1870e+24, grad_fn=<MulBackward0>)\n",
      "tensor(9.9500e+22, grad_fn=<MulBackward0>)\n",
      "tensor(3.4141e+17, grad_fn=<MulBackward0>)\n",
      "tensor(1.7162e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.1543e+21, grad_fn=<MulBackward0>)\n",
      "tensor(6.2342e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.2095e+19, grad_fn=<MulBackward0>)\n",
      "tensor(3.5309e+24, grad_fn=<MulBackward0>)\n",
      "tensor(9.3626e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.7549e+18, grad_fn=<MulBackward0>)\n",
      "tensor(6.9516e+21, grad_fn=<MulBackward0>)\n",
      "tensor(5.9981e+23, grad_fn=<MulBackward0>)\n",
      "tensor(5.5620e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.2638e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.1094e+26, grad_fn=<MulBackward0>)\n",
      "tensor(9.2333e+23, grad_fn=<MulBackward0>)\n",
      "tensor(9.2621e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3649255.7500, grad_fn=<MulBackward0>)\n",
      "tensor(1.2384e+27, grad_fn=<MulBackward0>)\n",
      "tensor(23388844., grad_fn=<MulBackward0>)\n",
      "tensor(163.2107, grad_fn=<MulBackward0>)\n",
      "tensor(6.6858e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.4150e+22, grad_fn=<MulBackward0>)\n",
      "tensor(6.2223e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.2563e+20, grad_fn=<MulBackward0>)\n",
      "tensor(8.5932e+25, grad_fn=<MulBackward0>)\n",
      "tensor(97.8604, grad_fn=<MulBackward0>)\n",
      "tensor(1.7516e+13, grad_fn=<MulBackward0>)\n",
      "tensor(-1.7573e+08, grad_fn=<MulBackward0>)\n",
      "tensor(286.2733, grad_fn=<MulBackward0>)\n",
      "tensor(1.5905e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.9279e+27, grad_fn=<MulBackward0>)\n",
      "tensor(9.5200e+23, grad_fn=<MulBackward0>)\n",
      "tensor(8.4107e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.8175e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.0077e+24, grad_fn=<MulBackward0>)\n",
      "tensor(5.9228e+18, grad_fn=<MulBackward0>)\n",
      "tensor(-78601.2188, grad_fn=<MulBackward0>)\n",
      "tensor(2.1506e+26, grad_fn=<MulBackward0>)\n",
      "tensor(-417224.3438, grad_fn=<MulBackward0>)\n",
      "tensor(1.4953e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.4495e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.3689e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.0538e+22, grad_fn=<MulBackward0>)\n",
      "tensor(4.3370e+20, grad_fn=<MulBackward0>)\n",
      "tensor(2.3317e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.7748e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.8151e+18, grad_fn=<MulBackward0>)\n",
      "tensor(2.0409e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.0944e+12, grad_fn=<MulBackward0>)\n",
      "tensor(7.7700e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1867114., grad_fn=<MulBackward0>)\n",
      "tensor(1.1101e+27, grad_fn=<MulBackward0>)\n",
      "tensor(5.2918e+12, grad_fn=<MulBackward0>)\n",
      "tensor(2.4641e+25, grad_fn=<MulBackward0>)\n",
      "tensor(-3.1966e+10, grad_fn=<MulBackward0>)\n",
      "tensor(3.1505e+21, grad_fn=<MulBackward0>)\n",
      "tensor(3.4771e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.0424e+20, grad_fn=<MulBackward0>)\n",
      "tensor(2.9225e+23, grad_fn=<MulBackward0>)\n",
      "tensor(8.4922e+23, grad_fn=<MulBackward0>)\n",
      "tensor(44803.3086, grad_fn=<MulBackward0>)\n",
      "tensor(6.7408e+23, grad_fn=<MulBackward0>)\n",
      "tensor(3.6552e+19, grad_fn=<MulBackward0>)\n",
      "tensor(-2.1901e+10, grad_fn=<MulBackward0>)\n",
      "tensor(1.0416e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.6662e+15, grad_fn=<MulBackward0>)\n",
      "tensor(1.3063e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.3282e+15, grad_fn=<MulBackward0>)\n",
      "tensor(206.6740, grad_fn=<MulBackward0>)\n",
      "tensor(1.9219e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.6153e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.9623e+19, grad_fn=<MulBackward0>)\n",
      "tensor(1.7593e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.5388e+25, grad_fn=<MulBackward0>)\n",
      "tensor(9.6438e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.1879e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.9215e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.4227e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.2627e+18, grad_fn=<MulBackward0>)\n",
      "tensor(2.2400e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.8747e+26, grad_fn=<MulBackward0>)\n",
      "tensor(-21.2373, grad_fn=<MulBackward0>)\n",
      "tensor(2.9044e+21, grad_fn=<MulBackward0>)\n",
      "tensor(2.7518e+21, grad_fn=<MulBackward0>)\n",
      "tensor(2.5697e+24, grad_fn=<MulBackward0>)\n",
      "tensor(9.7734e+23, grad_fn=<MulBackward0>)\n",
      "tensor(4.7860e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.3708e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.4941e+24, grad_fn=<MulBackward0>)\n",
      "tensor(3.5600e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.5090e+25, grad_fn=<MulBackward0>)\n",
      "tensor(9.7497e+24, grad_fn=<MulBackward0>)\n",
      "tensor(9.2096e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.1293e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.1223e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.4224e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.9867e+21, grad_fn=<MulBackward0>)\n",
      "tensor(3.9971e+14, grad_fn=<MulBackward0>)\n",
      "tensor(2.2395e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.7367e+23, grad_fn=<MulBackward0>)\n",
      "tensor(3.4365e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.8307e+22, grad_fn=<MulBackward0>)\n",
      "tensor(2.3512e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.7971e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.2892e+18, grad_fn=<MulBackward0>)\n",
      "tensor(5.9536e+15, grad_fn=<MulBackward0>)\n",
      "tensor(6.3502e+26, grad_fn=<MulBackward0>)\n",
      "tensor(5.4767e+08, grad_fn=<MulBackward0>)\n",
      "tensor(1.0343e+24, grad_fn=<MulBackward0>)\n",
      "tensor(2.0651e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.1770e+13, grad_fn=<MulBackward0>)\n",
      "tensor(5.6266e+16, grad_fn=<MulBackward0>)\n",
      "tensor(4.9699e+14, grad_fn=<MulBackward0>)\n",
      "tensor(2.9474e+19, grad_fn=<MulBackward0>)\n",
      "tensor(1.2948e+24, grad_fn=<MulBackward0>)\n",
      "tensor(-65749148., grad_fn=<MulBackward0>)\n",
      "tensor(21.3224, grad_fn=<MulBackward0>)\n",
      "tensor(2.1167e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.2534e+19, grad_fn=<MulBackward0>)\n",
      "tensor(3.6054e+22, grad_fn=<MulBackward0>)\n",
      "tensor(2.0146e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.7215e+23, grad_fn=<MulBackward0>)\n",
      "tensor(5.1472e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.0445e+25, grad_fn=<MulBackward0>)\n",
      "tensor(5.8313e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.1514e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.1910e+15, grad_fn=<MulBackward0>)\n",
      "tensor(6.2195e+15, grad_fn=<MulBackward0>)\n",
      "tensor(3.3986e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.0451e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.5939e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.3137e+22, grad_fn=<MulBackward0>)\n",
      "tensor(4.3276e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.1147e+10, grad_fn=<MulBackward0>)\n",
      "tensor(6.7008e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.4654e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.8337e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.9036e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.1664e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.6158e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.3686e+15, grad_fn=<MulBackward0>)\n",
      "tensor(1.4915e+24, grad_fn=<MulBackward0>)\n",
      "tensor(8.9695e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.3974e+10, grad_fn=<MulBackward0>)\n",
      "tensor(2.4556e+22, grad_fn=<MulBackward0>)\n",
      "tensor(5.8370e+22, grad_fn=<MulBackward0>)\n",
      "tensor(3.3605e+27, grad_fn=<MulBackward0>)\n",
      "tensor(9.0176e+24, grad_fn=<MulBackward0>)\n",
      "tensor(8.6648e+15, grad_fn=<MulBackward0>)\n",
      "tensor(1.6926e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.1014e+27, grad_fn=<MulBackward0>)\n",
      "tensor(8.8237e+25, grad_fn=<MulBackward0>)\n",
      "tensor(5.0356e+27, grad_fn=<MulBackward0>)\n",
      "tensor(9.7263e+21, grad_fn=<MulBackward0>)\n",
      "tensor(5.2043e+17, grad_fn=<MulBackward0>)\n",
      "tensor(2.4533e+15, grad_fn=<MulBackward0>)\n",
      "tensor(5.7138e+25, grad_fn=<MulBackward0>)\n",
      "tensor(5.6578e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.5322e+21, grad_fn=<MulBackward0>)\n",
      "tensor(5.2059e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.4160e+21, grad_fn=<MulBackward0>)\n",
      "tensor(2.5271e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.0104e+24, grad_fn=<MulBackward0>)\n",
      "tensor(4.8701e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4505.4385, grad_fn=<MulBackward0>)\n",
      "tensor(5.0755e+24, grad_fn=<MulBackward0>)\n",
      "tensor(7.6926e+19, grad_fn=<MulBackward0>)\n",
      "tensor(-3.6683e+10, grad_fn=<MulBackward0>)\n",
      "tensor(1.1215e+15, grad_fn=<MulBackward0>)\n",
      "tensor(6.2210e+14, grad_fn=<MulBackward0>)\n",
      "tensor(5.2483e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.4924e+21, grad_fn=<MulBackward0>)\n",
      "tensor(-2.7829e+08, grad_fn=<MulBackward0>)\n",
      "tensor(4.2600e+27, grad_fn=<MulBackward0>)\n",
      "tensor(-22375430., grad_fn=<MulBackward0>)\n",
      "tensor(5.2051e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.2746e+22, grad_fn=<MulBackward0>)\n",
      "tensor(6.3461e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.5411e+18, grad_fn=<MulBackward0>)\n",
      "tensor(3.5565e+25, grad_fn=<MulBackward0>)\n",
      "tensor(7.6331e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.2857e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.0241e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.7410e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.6765e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.0989e+23, grad_fn=<MulBackward0>)\n",
      "tensor(4.1338e+13, grad_fn=<MulBackward0>)\n",
      "tensor(4.4628e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.7119e+27, grad_fn=<MulBackward0>)\n",
      "tensor(9.5827e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.6819e+24, grad_fn=<MulBackward0>)\n",
      "tensor(4.4706e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.8608e+25, grad_fn=<MulBackward0>)\n",
      "tensor(-6.5851e+08, grad_fn=<MulBackward0>)\n",
      "tensor(8.2329e+27, grad_fn=<MulBackward0>)\n",
      "tensor(5.8857e+26, grad_fn=<MulBackward0>)\n",
      "tensor(-1.0869e+08, grad_fn=<MulBackward0>)\n",
      "tensor(1.2842e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.2244e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.7539e+26, grad_fn=<MulBackward0>)\n",
      "tensor(-2.2347e+10, grad_fn=<MulBackward0>)\n",
      "tensor(7.4518e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.8380e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.6806e+19, grad_fn=<MulBackward0>)\n",
      "tensor(1.0635e+26, grad_fn=<MulBackward0>)\n",
      "tensor(9.2349e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.8158e+15, grad_fn=<MulBackward0>)\n",
      "tensor(3.9555e+14, grad_fn=<MulBackward0>)\n",
      "tensor(3.8380e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.5515e+27, grad_fn=<MulBackward0>)\n",
      "tensor(9.0673e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.3452e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.0036e+26, grad_fn=<MulBackward0>)\n",
      "tensor(9.0306e+22, grad_fn=<MulBackward0>)\n",
      "tensor(432.2423, grad_fn=<MulBackward0>)\n",
      "tensor(2.2971e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.7424e+08, grad_fn=<MulBackward0>)\n",
      "tensor(450583.1562, grad_fn=<MulBackward0>)\n",
      "tensor(1.7071e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.3347e+24, grad_fn=<MulBackward0>)\n",
      "tensor(3.7559e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.2827e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.5278e+13, grad_fn=<MulBackward0>)\n",
      "tensor(1.7338e+18, grad_fn=<MulBackward0>)\n",
      "tensor(9.9487e+20, grad_fn=<MulBackward0>)\n",
      "tensor(-11812.2344, grad_fn=<MulBackward0>)\n",
      "tensor(12898736., grad_fn=<MulBackward0>)\n",
      "tensor(6.6820e+18, grad_fn=<MulBackward0>)\n",
      "tensor(5.6979e+20, grad_fn=<MulBackward0>)\n",
      "tensor(5.6656e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.1633e+22, grad_fn=<MulBackward0>)\n",
      "tensor(9.3612e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.0117e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.0718e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.1941e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.0296e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.5229e+14, grad_fn=<MulBackward0>)\n",
      "tensor(3.5556e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.7755e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.6785e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.1119e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.0183e+23, grad_fn=<MulBackward0>)\n",
      "tensor(4.0837e+16, grad_fn=<MulBackward0>)\n",
      "tensor(4.3544e+11, grad_fn=<MulBackward0>)\n",
      "tensor(4.6580e+25, grad_fn=<MulBackward0>)\n",
      "tensor(8.8990e+26, grad_fn=<MulBackward0>)\n",
      "tensor(9.4735e+24, grad_fn=<MulBackward0>)\n",
      "tensor(8.5488e+18, grad_fn=<MulBackward0>)\n",
      "tensor(2.5049e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.1282e+16, grad_fn=<MulBackward0>)\n",
      "tensor(4.9894e+09, grad_fn=<MulBackward0>)\n",
      "tensor(2.4186e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.5343e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.8273e+26, grad_fn=<MulBackward0>)\n",
      "tensor(139.0234, grad_fn=<MulBackward0>)\n",
      "tensor(6.1289e+21, grad_fn=<MulBackward0>)\n",
      "tensor(4.0010e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.5202e+19, grad_fn=<MulBackward0>)\n",
      "tensor(2.9593e+17, grad_fn=<MulBackward0>)\n",
      "tensor(8.4541e+22, grad_fn=<MulBackward0>)\n",
      "tensor(9.1903e+13, grad_fn=<MulBackward0>)\n",
      "tensor(3.1383e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.5974e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.3770e+19, grad_fn=<MulBackward0>)\n",
      "tensor(8.1667e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.0585e+21, grad_fn=<MulBackward0>)\n",
      "tensor(4.0610e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.0662e+19, grad_fn=<MulBackward0>)\n",
      "tensor(1.7995e+26, grad_fn=<MulBackward0>)\n",
      "tensor(9.4757e+16, grad_fn=<MulBackward0>)\n",
      "tensor(-6821666., grad_fn=<MulBackward0>)\n",
      "tensor(4.5274e+17, grad_fn=<MulBackward0>)\n",
      "tensor(1.3521e+19, grad_fn=<MulBackward0>)\n",
      "tensor(4.7525e+20, grad_fn=<MulBackward0>)\n",
      "tensor(3.6135e+23, grad_fn=<MulBackward0>)\n",
      "tensor(5.9148e+24, grad_fn=<MulBackward0>)\n",
      "tensor(7.8845e+27, grad_fn=<MulBackward0>)\n",
      "tensor(9.3173e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.1861e+14, grad_fn=<MulBackward0>)\n",
      "tensor(2.4357e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.1935e+24, grad_fn=<MulBackward0>)\n",
      "tensor(-9.8568e+08, grad_fn=<MulBackward0>)\n",
      "tensor(1.9665e+13, grad_fn=<MulBackward0>)\n",
      "tensor(5.2863e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.5379e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.9828e+26, grad_fn=<MulBackward0>)\n",
      "tensor(6.1980e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.7232e+20, grad_fn=<MulBackward0>)\n",
      "tensor(3.4703e+10, grad_fn=<MulBackward0>)\n",
      "tensor(7.9980e+24, grad_fn=<MulBackward0>)\n",
      "tensor(5.8760e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.1389e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.1024e+25, grad_fn=<MulBackward0>)\n",
      "tensor(9.3574e+26, grad_fn=<MulBackward0>)\n",
      "tensor(6.1664e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2276.2224, grad_fn=<MulBackward0>)\n",
      "tensor(2.2150e+17, grad_fn=<MulBackward0>)\n",
      "tensor(9408.7627, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4871e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.5027e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.4640e+26, grad_fn=<MulBackward0>)\n",
      "tensor(4.5038e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.8245e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.5260e+20, grad_fn=<MulBackward0>)\n",
      "tensor(4.8215e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.0492e+17, grad_fn=<MulBackward0>)\n",
      "tensor(5.9708e+25, grad_fn=<MulBackward0>)\n",
      "tensor(-3592356.2500, grad_fn=<MulBackward0>)\n",
      "tensor(7.0995e+10, grad_fn=<MulBackward0>)\n",
      "tensor(1.9793e+22, grad_fn=<MulBackward0>)\n",
      "tensor(4.9190e+17, grad_fn=<MulBackward0>)\n",
      "tensor(9.6436e+14, grad_fn=<MulBackward0>)\n",
      "tensor(1.3684e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.0410e+12, grad_fn=<MulBackward0>)\n",
      "tensor(8.7224e+22, grad_fn=<MulBackward0>)\n",
      "tensor(7.7547e+25, grad_fn=<MulBackward0>)\n",
      "tensor(9.4400e+21, grad_fn=<MulBackward0>)\n",
      "tensor(3.7583e+11, grad_fn=<MulBackward0>)\n",
      "tensor(9.9428e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.1338e+23, grad_fn=<MulBackward0>)\n",
      "tensor(6.5366e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.0491e+22, grad_fn=<MulBackward0>)\n",
      "tensor(8.3425e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.1312e+22, grad_fn=<MulBackward0>)\n",
      "tensor(5.1846e+15, grad_fn=<MulBackward0>)\n",
      "tensor(8.5933e+17, grad_fn=<MulBackward0>)\n",
      "tensor(4.2038e+23, grad_fn=<MulBackward0>)\n",
      "tensor(3.7024e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.9727e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.1410e+27, grad_fn=<MulBackward0>)\n",
      "tensor(5.1214e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.2096e+25, grad_fn=<MulBackward0>)\n",
      "tensor(-5.7564e+09, grad_fn=<MulBackward0>)\n",
      "tensor(-891924., grad_fn=<MulBackward0>)\n",
      "tensor(4.1014e+18, grad_fn=<MulBackward0>)\n",
      "tensor(2.8490e+26, grad_fn=<MulBackward0>)\n",
      "tensor(8.7807e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.9558e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.1841e+21, grad_fn=<MulBackward0>)\n",
      "tensor(2.6976e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.0944e+24, grad_fn=<MulBackward0>)\n",
      "tensor(2.6369e+22, grad_fn=<MulBackward0>)\n",
      "tensor(8.4115e+19, grad_fn=<MulBackward0>)\n",
      "tensor(9.9510e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.4850e+27, grad_fn=<MulBackward0>)\n",
      "tensor(212828.6094, grad_fn=<MulBackward0>)\n",
      "tensor(7.9712e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.2091e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.9672e+15, grad_fn=<MulBackward0>)\n",
      "tensor(1.0701e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.7760e+23, grad_fn=<MulBackward0>)\n",
      "tensor(5.1478e+11, grad_fn=<MulBackward0>)\n",
      "tensor(9.4378e+19, grad_fn=<MulBackward0>)\n",
      "tensor(4.7107e+27, grad_fn=<MulBackward0>)\n",
      "tensor(5.9968e+18, grad_fn=<MulBackward0>)\n",
      "tensor(1.2446e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.2587e+26, grad_fn=<MulBackward0>)\n",
      "tensor(9.3075e+19, grad_fn=<MulBackward0>)\n",
      "tensor(2.7030e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.6930e+17, grad_fn=<MulBackward0>)\n",
      "tensor(5.1704e+23, grad_fn=<MulBackward0>)\n",
      "tensor(-38845716., grad_fn=<MulBackward0>)\n",
      "tensor(1.2763e+20, grad_fn=<MulBackward0>)\n",
      "tensor(3.2095e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.0623e+23, grad_fn=<MulBackward0>)\n",
      "tensor(4.1292e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.0663e+24, grad_fn=<MulBackward0>)\n",
      "tensor(79.4904, grad_fn=<MulBackward0>)\n",
      "tensor(6.8849e+16, grad_fn=<MulBackward0>)\n",
      "tensor(6.5448e+24, grad_fn=<MulBackward0>)\n",
      "tensor(2.0546e+08, grad_fn=<MulBackward0>)\n",
      "tensor(5.4276e+13, grad_fn=<MulBackward0>)\n",
      "tensor(1.8920e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.2049e+19, grad_fn=<MulBackward0>)\n",
      "tensor(1.3982e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.5762e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.6671e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.5209e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.6572e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.9677e+11, grad_fn=<MulBackward0>)\n",
      "tensor(7.0114e+11, grad_fn=<MulBackward0>)\n",
      "tensor(2.4291e+23, grad_fn=<MulBackward0>)\n",
      "tensor(6.2385e+25, grad_fn=<MulBackward0>)\n",
      "tensor(9.6114e+25, grad_fn=<MulBackward0>)\n",
      "tensor(7.6024e+18, grad_fn=<MulBackward0>)\n",
      "tensor(8.0968e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.7606e+17, grad_fn=<MulBackward0>)\n",
      "tensor(3.2010e+23, grad_fn=<MulBackward0>)\n",
      "tensor(3.3212e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.9635e+25, grad_fn=<MulBackward0>)\n",
      "tensor(9.5428e+18, grad_fn=<MulBackward0>)\n",
      "tensor(3.2834e+19, grad_fn=<MulBackward0>)\n",
      "tensor(6.8694e+12, grad_fn=<MulBackward0>)\n",
      "tensor(6.8082e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.6985e+19, grad_fn=<MulBackward0>)\n",
      "tensor(7.7172e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.1436e+22, grad_fn=<MulBackward0>)\n",
      "tensor(3.2522e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.2696e+25, grad_fn=<MulBackward0>)\n",
      "tensor(-75.2210, grad_fn=<MulBackward0>)\n",
      "tensor(1.2069e+26, grad_fn=<MulBackward0>)\n",
      "tensor(20370.6230, grad_fn=<MulBackward0>)\n",
      "tensor(2.5810e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.2700e+21, grad_fn=<MulBackward0>)\n",
      "tensor(6.1846e+19, grad_fn=<MulBackward0>)\n",
      "tensor(6.6808e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.4266e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.8054e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.0303e+24, grad_fn=<MulBackward0>)\n",
      "tensor(5.4532e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.1089e+17, grad_fn=<MulBackward0>)\n",
      "tensor(3.0324e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.4055e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.1104e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.1522e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.0378e+22, grad_fn=<MulBackward0>)\n",
      "tensor(3.1277e+13, grad_fn=<MulBackward0>)\n",
      "tensor(1.2544e+27, grad_fn=<MulBackward0>)\n",
      "tensor(5.4672e+25, grad_fn=<MulBackward0>)\n",
      "tensor(3.6121e+24, grad_fn=<MulBackward0>)\n",
      "tensor(3.0852e+24, grad_fn=<MulBackward0>)\n",
      "tensor(4.9185e+12, grad_fn=<MulBackward0>)\n",
      "tensor(3.1935e+17, grad_fn=<MulBackward0>)\n",
      "tensor(2.6310e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.5575e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.6833e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.0538e+20, grad_fn=<MulBackward0>)\n",
      "tensor(3.3809e+08, grad_fn=<MulBackward0>)\n",
      "tensor(1.8582e+20, grad_fn=<MulBackward0>)\n",
      "tensor(2.6536e+25, grad_fn=<MulBackward0>)\n",
      "tensor(-1.3587e+10, grad_fn=<MulBackward0>)\n",
      "tensor(3.4724e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.3765e+09, grad_fn=<MulBackward0>)\n",
      "tensor(4.1627e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.7857e+16, grad_fn=<MulBackward0>)\n",
      "tensor(1.0727e+19, grad_fn=<MulBackward0>)\n",
      "tensor(3.4932e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.3780e+17, grad_fn=<MulBackward0>)\n",
      "tensor(5.6561e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.6727e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.0755e+23, grad_fn=<MulBackward0>)\n",
      "tensor(3.3383e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.1923e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.2595e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.5596e+19, grad_fn=<MulBackward0>)\n",
      "tensor(2.0846e+20, grad_fn=<MulBackward0>)\n",
      "tensor(5.9523e+21, grad_fn=<MulBackward0>)\n",
      "tensor(4.4467e+24, grad_fn=<MulBackward0>)\n",
      "tensor(7.4466e+15, grad_fn=<MulBackward0>)\n",
      "tensor(2.2983e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.8031e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.0728e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.7565e+22, grad_fn=<MulBackward0>)\n",
      "tensor(5.2218e+10, grad_fn=<MulBackward0>)\n",
      "tensor(6947330., grad_fn=<MulBackward0>)\n",
      "tensor(-4.0495e+10, grad_fn=<MulBackward0>)\n",
      "tensor(2.4347e+27, grad_fn=<MulBackward0>)\n",
      "tensor(6.6854e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.1830e+22, grad_fn=<MulBackward0>)\n",
      "tensor(3.8657e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.3728e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.9126e+17, grad_fn=<MulBackward0>)\n",
      "tensor(5.0939e+22, grad_fn=<MulBackward0>)\n",
      "tensor(2.3329e+21, grad_fn=<MulBackward0>)\n",
      "tensor(7.1438e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.0435e+27, grad_fn=<MulBackward0>)\n",
      "tensor(8.9923e+23, grad_fn=<MulBackward0>)\n",
      "tensor(3.0846e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.6494e+17, grad_fn=<MulBackward0>)\n",
      "tensor(6.9357e+12, grad_fn=<MulBackward0>)\n",
      "tensor(8.9979e+16, grad_fn=<MulBackward0>)\n",
      "tensor(2.3574e+22, grad_fn=<MulBackward0>)\n",
      "tensor(8.1411e+20, grad_fn=<MulBackward0>)\n",
      "tensor(2.3639e+24, grad_fn=<MulBackward0>)\n",
      "tensor(3.9823e+27, grad_fn=<MulBackward0>)\n",
      "tensor(7.2320e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.2451e+25, grad_fn=<MulBackward0>)\n",
      "tensor(5.5048e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.3805e+27, grad_fn=<MulBackward0>)\n",
      "tensor(3.7025e+26, grad_fn=<MulBackward0>)\n",
      "tensor(-1825527.2500, grad_fn=<MulBackward0>)\n",
      "tensor(1.9041e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.4751e+15, grad_fn=<MulBackward0>)\n",
      "tensor(2.1025e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.2521e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.6575e+26, grad_fn=<MulBackward0>)\n",
      "tensor(6.9236e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.2111e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.5917e+26, grad_fn=<MulBackward0>)\n",
      "tensor(3.9589e+23, grad_fn=<MulBackward0>)\n",
      "tensor(1.9014e+19, grad_fn=<MulBackward0>)\n",
      "tensor(1.8609e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.0783e+19, grad_fn=<MulBackward0>)\n",
      "tensor(6.8829e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.8255e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.6760e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.2705e+27, grad_fn=<MulBackward0>)\n",
      "tensor(5.0912e+25, grad_fn=<MulBackward0>)\n",
      "tensor(1.2957e+17, grad_fn=<MulBackward0>)\n",
      "tensor(3.2904e+26, grad_fn=<MulBackward0>)\n",
      "tensor(1.7645e+22, grad_fn=<MulBackward0>)\n",
      "tensor(3.3502e+24, grad_fn=<MulBackward0>)\n",
      "tensor(6.9381e+19, grad_fn=<MulBackward0>)\n",
      "tensor(1.5236e+22, grad_fn=<MulBackward0>)\n",
      "tensor(-1.0386e+10, grad_fn=<MulBackward0>)\n",
      "tensor(1.5829e+25, grad_fn=<MulBackward0>)\n",
      "tensor(2.1275e+22, grad_fn=<MulBackward0>)\n",
      "tensor(8.1436e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.3086e+25, grad_fn=<MulBackward0>)\n",
      "tensor(9.7029e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.9972e+27, grad_fn=<MulBackward0>)\n",
      "tensor(1.0042e+11, grad_fn=<MulBackward0>)\n",
      "tensor(4.0091e+24, grad_fn=<MulBackward0>)\n",
      "tensor(1.0465e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.6415e+27, grad_fn=<MulBackward0>)\n",
      "tensor(2.5984e+14, grad_fn=<MulBackward0>)\n",
      "tensor(8.8899e+20, grad_fn=<MulBackward0>)\n",
      "tensor(1.9144e+18, grad_fn=<MulBackward0>)\n",
      "tensor(-1521.5970, grad_fn=<MulBackward0>)\n",
      "tensor(4.5557e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.8603e+20, grad_fn=<MulBackward0>)\n",
      "tensor(7.2085e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.0476e+17, grad_fn=<MulBackward0>)\n",
      "tensor(3.9056e+26, grad_fn=<MulBackward0>)\n",
      "tensor(2.1426e+25, grad_fn=<MulBackward0>)\n",
      "tensor(8.0838e+16, grad_fn=<MulBackward0>)\n",
      "tensor(7.5084e+21, grad_fn=<MulBackward0>)\n",
      "tensor(1.3867e+20, grad_fn=<MulBackward0>)\n",
      "tensor(8.2318e+25, grad_fn=<MulBackward0>)\n",
      "tensor(6.0844e+14, grad_fn=<MulBackward0>)\n",
      "tensor(1.1726e+23, grad_fn=<MulBackward0>)\n",
      "tensor(2.0061e+25, grad_fn=<MulBackward0>)\n",
      "tensor(4.1735e+18, grad_fn=<MulBackward0>)\n",
      "tensor(2.8952e+27, grad_fn=<MulBackward0>)\n",
      "tensor(4.9288e+24, grad_fn=<MulBackward0>)\n",
      "tensor(7.7590e+14, grad_fn=<MulBackward0>)\n",
      "tensor(3.2591e+22, grad_fn=<MulBackward0>)\n",
      "tensor(1.2509e+23, grad_fn=<MulBackward0>)\n",
      "\n",
      "Weights: \n",
      "10:1.00 \n",
      " 9:1.00  \n",
      " 8:1.00  \n",
      " 7:1.00  \n",
      " 6:1.00  \n",
      " 5:1.00  \n",
      " 4:1.00  \n",
      " 3:1.00  \n",
      " 2:1.00  \n",
      " 1:1.00  \n",
      " 0:1.00  \n"
     ]
    }
   ],
   "source": [
    "for p in x[:,1]:\n",
    "    z = func_actual(p)\n",
    "    z_pred = func_predict(p, training_weights)\n",
    "    print(z * z_pred)\n",
    "    loss = torch.sigmoid(z - z_pred) * 500\n",
    "    zeroGrad(x)\n",
    "    zeroGrad(z)\n",
    "    zeroGrad(z_pred)\n",
    "    zeroGrad(training_weights)\n",
    "    loss.backward()\n",
    "    training_weights.data.add_(-lr, training_weights.grad.data)\n",
    "    \n",
    "printWeights(training_weights)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-af1d3a9cac4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mz_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mzeroAllGrads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-17c4a7700d98>\u001b[0m in \u001b[0;36mlossFunction\u001b[1;34m(Y, Y_pred)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"len() of a 0-d tensor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of a 0-d tensor"
     ]
    }
   ],
   "source": [
    "z_pred = func_predict(x[:,1], training_weights)\n",
    "loss = criterion(z, z_pred)\n",
    "print(loss)\n",
    "zeroAllGrads()\n",
    "loss.backward()\n",
    "# print(x.grad.data.numpy())\n",
    "print(training_weights.grad.data.numpy())\n",
    "training_weights.data.add_(-lr, training_weights.grad.data)\n",
    "printWeights(training_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred = func_predict(x[:,1], training_weights)\n",
    "loss = criterion(z, z_pred)\n",
    "print(loss)\n",
    "zeroAllGrads()\n",
    "loss.backward()\n",
    "# print(x.grad.data.numpy())\n",
    "print(training_weights.grad.data.numpy())\n",
    "training_weights.data.add_(-lr, training_weights.grad.data)\n",
    "printWeights(training_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred = func_predict(x[:,1], training_weights)\n",
    "loss = criterion(z, z_pred)\n",
    "print(loss)\n",
    "zeroAllGrads()\n",
    "loss.backward()\n",
    "# print(x.grad.data.numpy())\n",
    "print(training_weights.grad.data.numpy())\n",
    "training_weights.data.add_(-lr, training_weights.grad.data)\n",
    "printWeights(training_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred = func_predict(x, training_weights)\n",
    "loss = criterion(z, z_pred)\n",
    "print(loss)\n",
    "zeroAllGrads()\n",
    "loss.backward()\n",
    "# print(x.grad.data.numpy())\n",
    "print(training_weights.grad.data.numpy())\n",
    "training_weights.data.add_(-lr, training_weights.grad.data)\n",
    "printWeights(training_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred = func_predict(x, training_weights)\n",
    "loss = criterion(z, z_pred)\n",
    "print(loss)\n",
    "zeroAllGrads()\n",
    "loss.backward()\n",
    "print(training_weights.grad.data.numpy())\n",
    "training_weights.data.add_(-lr, training_weights.grad.data)\n",
    "printWeights(training_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred = func_predict(x, training_weights)\n",
    "loss = criterion(z, z_pred)\n",
    "print(loss)\n",
    "zeroAllGrads()\n",
    "loss.backward()\n",
    "print(training_weights.grad.data.numpy())\n",
    "training_weights.data.add_(-lr, training_weights.grad.data)\n",
    "printWeights(training_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
