{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to help build a Training Loop class\n",
    "\n",
    "We will use the existing **modules.lib.ChextXRayImages** class to obtain the DataFrames, Datasets and Loader for the CheXpert dataset.\n",
    "\n",
    "We will create a dummy NN model just to validate that our training loop is good.  \n",
    "\n",
    "We will use the *n_random_rows* parameter in our Loaders class to make the training loops quick.\n",
    "\n",
    "#### We want to flesh out a few things in this notebook to help us build the class:\n",
    "- Make sure out model output and our loss function are compatible (i.e. who does the sigmoid/softmax)\n",
    "- Find an adequate way to display a helpful \"accuracy\" score on the completion of each epoch\n",
    "- Build objects that hold the history of the training (loss, weights, predictions)\n",
    "- Add the ability to tag the predictions back to the original DataFrame via the ImageID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os, os.path\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd() ,'/modules'))\n",
    "root_path = \"C:/git/Springboard-Public/Capstone Project 2/\"\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_path = \"/content/drive/My Drive/Capstone Project 2/\"\n",
    "\n",
    "print('Current Working Dir: ', os.getcwd())\n",
    "print('Root Path: ', root_path)\n",
    "\n",
    "# We need to set the working directory since we are using relative paths from various locations\n",
    "if os.getcwd() != root_path:\n",
    "  os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from modules.lib.ChextXRayImages import *\n",
    "from modules.models.CustomPneumonia import CustomPneumoniaNN\n",
    "\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's run in cuda so we can catch things like:\n",
    "TypeError: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on device=cuda\n"
     ]
    }
   ],
   "source": [
    "force_cpu = True\n",
    "device = torch.device('cuda' if ~force_cpu and torch.cuda.is_available() else 'cpu')\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(f'Working on device={device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modules.lib.ChextXRayImages\n",
    "\n",
    "We will use this class to get both training and validation data loaders.\n",
    "\n",
    "We also want to pull the list of target columns and the 2 DataFames used in the loaders.\n",
    "\n",
    "The latter will be used to join our prediction values so that we know which predictions go with which x-rays.\n",
    "\n",
    "*Note:  The Loaders class will display a warning if there is more than a 2% difference in feature occurrence between train and validation.  Since we are using a very small sample of the full set of rows, we would expect warning on one or a few features.  As the number of rows in the sample goes up, the odds of getting an imbalance warning become fairly low.*\n",
    "\n",
    "*See the bottom of EDA.ipynb for more details*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\git\\Springboard-Public\\Capstone Project 2\\modules\\lib\\ChextXRayImages.py:244: UserWarning: \n",
      "Feature Imbalance Detected (train % - val %):\n",
      "   Lung_Opacity: 3.24%\n",
      "   Pneumothorax: 2.30%\n",
      "\n",
      "  self.warnFeatureImbalance(train, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Batches: 53\n",
      "Number of Validation Batches: 10\n",
      "Number of Training Images: 848\n",
      "Number of Validation Images: 160\n"
     ]
    }
   ],
   "source": [
    "loaders = Loaders()\n",
    "batch_size=16\n",
    "val_percent=0.15\n",
    "number_images = 1000\n",
    "train_loader, val_loader = loaders.getDataTrainValidateLoaders(batch_size=batch_size, \n",
    "                                                                        val_percent=val_percent, \n",
    "                                                                        n_random_rows=number_images)\n",
    "\n",
    "target_columns = loaders.target_columns\n",
    "\n",
    "train_actual = loaders.train_df\n",
    "val_actual = loaders.val_df\n",
    "\n",
    "print(f'Number of Training Batches: {len(train_loader):,}')\n",
    "print(f'Number of Validation Batches: {len(val_loader):,}')\n",
    "print(f'Number of Training Images: {len(train_loader) * batch_size:,}')\n",
    "print(f'Number of Validation Images: {len(val_loader) * batch_size:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple FC only model used just for testing\n",
    "\n",
    "We do not expect any kind of usable results from this model.  \n",
    "\n",
    "The biggest aspect here is the output shape and any final activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)       \n",
    "        self.flattened_length_ = 1*320*320\n",
    "        self.fc1 = nn.Linear(self.flattened_length_, 12)\n",
    "       \n",
    "    def forward(self, x):    \n",
    "        x = x.view(-1, self.flattened_length_)    \n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 12]       1,228,812\n",
      "       SimpleModel-2                   [-1, 12]               0\n",
      "================================================================\n",
      "Total params: 1,228,812\n",
      "Trainable params: 1,228,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 4.69\n",
      "Estimated Total Size (MB): 5.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = SimpleModel()\n",
    "\n",
    "net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "\n",
    "summary(net, (1, 320, 320)) #Known Harded code size generated by data loaders (todo: make attribute of loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at output from Loaders\n",
    "\n",
    "We just need to look at the first x-ray from the loader.\n",
    "\n",
    "#### We are looking here for:\n",
    "- What structure the loader gives us\n",
    "- The shapes of these objects\n",
    "- The shape and content of the output from our test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ImageIDs:  [204526 111147  42219 119536 189227 188489 164619 179090 143958 187657\n",
      "  12808 124342 146833 200985 117203 199989]\n",
      "\n",
      "--------------------------------------------------\n",
      "labels shape (batch size, feature count):  torch.Size([16, 12])\n",
      "inputs shape (batch size, channels, w, h):  torch.Size([16, 1, 320, 320])\n",
      "outputs shape (batch size, feature count):  torch.Size([16, 12])\n",
      "\n",
      "--------------------------------------------------\n",
      "labels:\n",
      " tensor([[0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
      "\n",
      "--------------------------------------------------\n",
      "model output:\n",
      " tensor([[-5.4050e-01, -2.1829e+00, -5.2545e-01,  7.9043e-01,  9.9097e-01,\n",
      "          5.9156e-01,  7.2286e-01,  1.4564e+00, -7.0828e-01,  1.1132e-01,\n",
      "          3.1443e+00, -1.0532e+00],\n",
      "        [ 3.4832e-01, -1.5099e+00, -1.1358e+00, -6.4011e-02, -1.2930e+00,\n",
      "          1.2993e-01,  7.1878e-01,  1.9128e+00, -6.9306e-01,  3.4137e-01,\n",
      "          7.7032e-01, -1.9656e-01],\n",
      "        [ 4.6707e-01, -1.0727e+00, -5.1383e-01, -1.7999e-01,  4.9211e-01,\n",
      "         -5.1270e-01,  8.6886e-01,  8.3708e-01, -2.3225e+00,  1.5703e+00,\n",
      "          1.1941e+00,  5.3728e-01],\n",
      "        [-1.0440e-01, -8.2334e-02, -4.5016e-01, -1.0879e+00,  4.1678e-01,\n",
      "          4.1869e+00, -6.1998e-01, -8.5050e-01,  9.4579e-01, -4.4467e-01,\n",
      "          2.0301e+00,  4.2836e-01],\n",
      "        [-3.0037e-02,  4.1921e-02, -4.5625e-01, -9.9651e-01,  1.7104e+00,\n",
      "          3.1747e+00, -1.1392e+00,  2.4233e-01,  5.6710e-01,  1.4128e+00,\n",
      "          3.0847e+00, -7.9101e-01],\n",
      "        [-2.7920e-01,  1.8105e+00, -4.4376e-02, -1.0234e+00, -4.8720e-01,\n",
      "          3.1662e+00, -1.3566e+00, -3.6303e-01,  2.0624e+00, -8.8355e-01,\n",
      "          2.1994e+00,  5.7008e-01],\n",
      "        [-1.7916e-01,  4.5990e-01,  7.3275e-01, -1.2801e+00,  9.9511e-01,\n",
      "          1.8507e+00, -1.2019e+00,  1.3489e-01,  5.0833e-01,  1.3290e+00,\n",
      "          2.0117e+00, -8.5404e-01],\n",
      "        [-1.1962e+00, -1.2124e+00, -2.2964e-01,  8.0102e-01,  1.1884e+00,\n",
      "          1.9324e+00, -1.6708e-03, -4.3194e-01, -1.0404e+00, -7.2104e-01,\n",
      "          2.6690e+00, -5.9698e-01],\n",
      "        [ 4.5873e-01, -4.5666e-01, -1.4073e+00, -2.4564e-01,  3.6530e-01,\n",
      "          8.8961e-01, -4.2278e-01,  1.1983e+00, -6.2405e-01,  4.9997e-01,\n",
      "          2.9173e+00, -9.9909e-01],\n",
      "        [-3.2158e-01, -2.0198e+00, -9.7982e-01,  5.0835e-01,  7.4654e-01,\n",
      "          1.0306e+00,  9.0765e-01,  1.9779e+00, -1.4596e+00,  2.1672e+00,\n",
      "          2.1690e+00,  5.3339e-01],\n",
      "        [-5.4547e-01,  1.1088e+00, -1.0870e+00, -1.5796e+00,  6.8220e-02,\n",
      "          2.2666e+00, -6.7373e-01, -1.1779e+00,  1.1331e+00, -6.3762e-01,\n",
      "          1.3738e+00, -6.9699e-01],\n",
      "        [-4.1438e-01, -1.0977e+00, -8.5810e-01,  9.9379e-01, -2.9877e-01,\n",
      "          1.9303e+00, -1.9724e-01,  2.3380e+00,  4.2121e-01,  1.2907e+00,\n",
      "         -8.0516e-01,  1.3479e+00],\n",
      "        [ 2.0569e-01, -8.2448e-01, -3.5963e-01, -3.8054e-01,  6.9165e-01,\n",
      "         -2.1848e-01,  6.6707e-01,  1.1078e+00, -1.6244e+00,  1.1095e-01,\n",
      "          1.3246e+00, -1.7990e+00],\n",
      "        [ 6.4942e-01, -4.1024e-01, -2.3336e-01, -9.6571e-01,  1.2796e+00,\n",
      "          9.2953e-01,  5.7353e-01,  1.2941e+00,  4.2905e-01,  7.4134e-01,\n",
      "          2.2052e+00, -7.1192e-01],\n",
      "        [ 3.1300e-01, -2.3273e+00, -3.9234e-01,  9.5772e-01,  1.4613e+00,\n",
      "          5.5585e-01, -2.2193e-01,  2.1018e+00, -1.7061e+00,  9.5763e-01,\n",
      "          1.9400e+00, -6.1368e-01],\n",
      "        [-5.1100e-01, -5.7849e-01, -8.3777e-01, -6.1422e-01, -1.5842e+00,\n",
      "          7.5982e-01,  1.1320e+00,  1.6583e+00, -2.6422e-01, -5.3654e-01,\n",
      "          1.2611e+00, -4.7036e-01]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "ImageID, inputs, labels = data['id'], data['img'], data['labels']\n",
    "\n",
    "print('Batch ImageIDs: ', ImageID.detach().numpy())\n",
    "\n",
    "print('\\n' + '-' * 50)\n",
    "\n",
    "# move data to device GPU OR CPU\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = net(inputs)\n",
    "\n",
    "print('labels shape (batch size, feature count): ', labels.shape)\n",
    "print('inputs shape (batch size, channels, w, h): ', inputs.shape)\n",
    "print('outputs shape (batch size, feature count): ', outputs.shape)\n",
    "\n",
    "print('\\n' + '-' * 50)\n",
    "\n",
    "print('labels:\\n', labels)\n",
    "\n",
    "print('\\n' + '-' * 50)\n",
    "\n",
    "print('model output:\\n', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, the labels and the model output have the same shape\n",
    "\n",
    "But the **labels are Boolean** and the model **output are Real** numbers.\n",
    "\n",
    "Our plan is to use **BCEWithLogitsLoss** as our loss function.  \n",
    "\n",
    "This function takes the **sigmoid** result for each of the outputed feature predictions and the performs **binary cross-entropy** for each of these squashed values.\n",
    "\n",
    "Because of this, we want the output to be Real values.\n",
    "\n",
    "*Note:  If down the road we want to try different loss functions, we may have to find a dynamic way to connect our model output to our loss function.  But for now, we will leave this \"hard coded\".*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Since our goal is to store all the interim losses and predictions for each epoch, we should be able to look at accuracy, recall and precision very methodically after the training completes.\n",
    "\n",
    "But we still need a way to monitor how well our model is performing in real-time.  We will be trying several different models.  We will probably want to change parameters on the model with the ability to abort the training if things aren't looking good.  So having a reliable indicator of accuracy during training is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Accuracy\n",
    "\n",
    "We have 12 features.  If we take $n$ x-rays, we will have $12n$ predictions.\n",
    "\n",
    "So a simple accuracy approach could be take all the percent of correct predictions:\n",
    "\n",
    "### $accuracy = \\frac{TotalCorrect}{12n}$\n",
    "\n",
    "But of we look at the EDA notebook, we will see that the actual positive rate for all but 3 of the features is 15% or less.\n",
    "\n",
    "This means that non-positive finding will dominate the accuracy score.\n",
    "\n",
    "i.e. A bad model will look pretty good at predicting non-positive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:\n",
      " tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "\n",
      "--------------------------------------------------\n",
      "Predicted:\n",
      " tensor([[0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.]], device='cuda:0')\n",
      "\n",
      "--------------------------------------------------\n",
      "Accurate Predictions:  89\n",
      "Total Predictions:  192\n",
      "Overall Accuracy: 46.35%\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "ImageID, inputs, labels = data['id'], data['img'], data['labels']\n",
    "\n",
    "# move data to device GPU OR CPU\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = net(inputs)\n",
    "\n",
    "# Since the model ouputs raw Real numbers, we need to convert the output to Boolean values\n",
    "\n",
    "predicted = torch.sigmoid(outputs.data) \n",
    "predicted[predicted >= 0.5] = 1 # assign 1 label to those with less than 0.5\n",
    "predicted[predicted < 0.5] = 0 # assign 0 label to those with less than 0.5\n",
    "\n",
    "print('Actual:\\n', labels)\n",
    "print('\\n' + '-' * 50)\n",
    "print('Predicted:\\n', predicted)\n",
    "\n",
    "print('\\n' + '-' * 50)\n",
    "\n",
    "train_batch_size, train_label_count = labels.shape\n",
    "\n",
    "print('Accurate Predictions: ', (predicted == labels).sum().item())\n",
    "print('Total Predictions: ', train_batch_size * train_label_count)\n",
    "train_acc = float((predicted == labels).sum()) / float((train_batch_size * train_label_count))\n",
    "print(f'Overall Accuracy: {train_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need a better score\n",
    "\n",
    "Our test model is probably pretty close to random choise, but it still shows an 80% accuracy because of the sparseness of positive findings in most of the features.\n",
    "\n",
    "Since we will want to look at scores like sensitivity, precision, F1 etc. when the training is done, so why not use some of these concepts in real-time output during training?\n",
    "\n",
    "#### Let's take a quick look at what we can do with SKLearn:\n",
    "\n",
    "We want to look at the **average** parmaeter for SKL's recall, precision and F1 to make sure we are using the right one for multi-label classification\n",
    "\n",
    "#### We first need to get an understanding how these scores work with multi-lable classification\n",
    "\n",
    "The primary parameter will look at for all 3 of these scores is **average**.  There are 5 options:\n",
    "- average=None\n",
    "- average='micro'\n",
    "- average='macro'\n",
    "- average='samples'\n",
    "- average='weighted'\n",
    "\n",
    "None will return a list of scores, one for each target.  All there other will return a scalar based on how the average is set.\n",
    "\n",
    "Since all 3 scores work the same, we will just look at recall.\n",
    "\n",
    "*Note:  Since F1 is just the harmonic meand of recall and precision, we will not show this value in our epoch output.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youci\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>True Positive Count</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Enlarged_Cardiomediastinum</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Lung_Opacity</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Lung_Lesion</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Edema</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Consolidation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Atelectasis</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Pleural_Effusion</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Pleural_Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Fracture</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Target  True Positive Count    Recall\n",
       "0   Enlarged_Cardiomediastinum                  0.0  0.000000\n",
       "1                 Cardiomegaly                  5.0  0.400000\n",
       "2                 Lung_Opacity                  6.0  0.166667\n",
       "3                  Lung_Lesion                  2.0  0.500000\n",
       "4                        Edema                  7.0  0.571429\n",
       "5                Consolidation                  0.0  0.000000\n",
       "6                    Pneumonia                  0.0  0.000000\n",
       "7                  Atelectasis                  1.0  1.000000\n",
       "8                 Pneumothorax                  1.0  0.000000\n",
       "9             Pleural_Effusion                  7.0  0.142857\n",
       "10               Pleural_Other                  0.0  0.000000\n",
       "11                    Fracture                  1.0  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual:\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Itemized avg 0.23174603174603173\n",
      "\n",
      "Itemized weighted avg 0.3333333333333333\n",
      "\n",
      "Itemized per row avg 0.19583333333333333\n",
      "\n",
      "\n",
      "SKLearn:\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "average=macro:  0.23174603174603173\n",
      "\n",
      "average=micro:  0.3333333333333333\n",
      "\n",
      "average=weighted:  0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youci\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average=samples:  0.1958333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, hamming_loss\n",
    "\n",
    "y_true = np.array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                   [0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
    "                   [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "                   [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "                   [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "                   [0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
    "                   [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
    "                   [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                   [0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
    "                   [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                   [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "                   [0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.]])\n",
    "\n",
    "y_pred = np.array([[1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0],\n",
    "                   [1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0],\n",
    "                   [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                   [1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1],\n",
    "                   [0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n",
    "                   [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "                   [1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n",
    "                   [0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1],\n",
    "                   [0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
    "                   [1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "                   [1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1],\n",
    "                   [0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0],\n",
    "                   [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0],\n",
    "                   [0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0]]) \n",
    "\n",
    "true_positive_count = y_true.sum(axis=0)\n",
    "\n",
    "# average=None will show a seperate score for each target\n",
    "itemized_recall = recall_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "\n",
    "# Let's put this \n",
    "df_itemized = pd.DataFrame({'Target':target_columns, \n",
    "                            'True Positive Count':true_positive_count, \n",
    "                            'Recall':itemized_recall})\n",
    "\n",
    "display(df_itemized)\n",
    "\n",
    "### Let's build these scores manually to see how they work ###\n",
    "\n",
    "#Average (macro)\n",
    "average_itemized_recall = np.mean(itemized_recall)\n",
    "\n",
    "#Weighted Average (micro and weighted)\n",
    "weighted_average_itemized_recall = sum(itemized_recall * true_positive_count) / true_positive_count.sum()\n",
    "   \n",
    "\"\"\"\n",
    "Per Row (sample):  \n",
    "This caclation differes from the rest.  \n",
    "The others caclulate the recall for all rows by target. They then average the target recall values.\n",
    "This one caculates the recall for all the targets by row (or samle).  It then averages the row recall values.              \n",
    "\"\"\"                   \n",
    "row_recalls = []\n",
    "for i in range(y_true.shape[0]):\n",
    "    tp=0\n",
    "    fn=0\n",
    "    for j in range(y_true.shape[1]):\n",
    "        t = y_true[i,j]\n",
    "        p = y_pred[i,j]\n",
    "        if t == 1 and p == 1:\n",
    "            tp+=1\n",
    "        if t == 1 and p == 0:\n",
    "            fn+=1\n",
    "    if (tp + fn) > 0:\n",
    "        row_recall = tp / (tp + fn)\n",
    "        row_recalls.append(row_recall)\n",
    "\n",
    "row_recalls = np.array(row_recalls)\n",
    "per_row_recall = row_recalls.sum() / y_true.shape[0]\n",
    "\n",
    "print('Manual:')\n",
    "print('\\n' + '-' * 50)\n",
    "print('\\nItemized avg',average_itemized_recall)\n",
    "print('\\nItemized weighted avg',weighted_average_itemized_recall)\n",
    "print('\\nItemized per row avg',per_row_recall)\n",
    "\n",
    "print('\\n\\nSKLearn:')\n",
    "print('\\n' + '-' * 50)\n",
    "print('\\naverage=macro: ',recall_score(y_true=y_true, y_pred=y_pred, average='macro'))\n",
    "print('\\naverage=micro: ',recall_score(y_true=y_true, y_pred=y_pred, average='micro'))\n",
    "print('\\naverage=weighted: ',recall_score(y_true=y_true, y_pred=y_pred, average='weighted'))\n",
    "print('\\naverage=samples: ',recall_score(y_true=y_true, y_pred=y_pred, average='samples'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also want to look at accuracy and Hamming loss\n",
    "\n",
    "SKLearn's accuracy_score is very unforgiving.  It only counts rows that have every category correct.\n",
    "\n",
    "So chances are, this score will not prove very useful, but it can still show us trends.\n",
    "\n",
    "The Hamming loss looks at each target independantly, so event partial label matches can contribute.\n",
    "\n",
    "*Note:  Hamming loss is negated, so we want the score to be as small as possible.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.0\n",
      "\n",
      "hamming_loss:  0.5\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score: ',accuracy_score(y_true=y_true, y_pred=y_pred, normalize=True))\n",
    "print('\\nhamming_loss: ',hamming_loss(y_true=y_true, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " array([[0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1],\n",
       "        [0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1],\n",
       "        [1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1],\n",
       "        [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1],\n",
       "        [1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = labels.cpu().data.numpy()\n",
    "\n",
    "y_pred = np.random.choice(a=[0, 1], size=predicted.shape)\n",
    "y_true, y_pred\n",
    "\n",
    "# itemized_recall = recall_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "# itemized_precision = precision_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "# itemized_f1 = f1_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "# tru_positive_count = y_true.sum(axis=0)\n",
    "# df_itemized = pd.DataFrame({'Target':target_columns, \n",
    "#                             'True Positive Count':tru_positive_count, \n",
    "#                             'Recall':itemized_recall, \n",
    "#                             'Precision':itemized_precision, \n",
    "#                             'F1':itemized_f1})\n",
    "# display(df_itemized)\n",
    "\n",
    "# sensitivity = recall_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "# precision = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "# f1 = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "\n",
    "# label_sensitivity = recall_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "# label_precision = precision_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "# label_f1 = f1_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "\n",
    "# print('Random:\\n' + '-' * 50)\n",
    "# print(label_sensitivity)\n",
    "# print(f'Sensitivity: {sensitivity:.2%}')\n",
    "# print(f'Precision: {precision:.2%}')\n",
    "# print(f'F1 (harmonic avarage): {f1:.2%}')\n",
    "\n",
    "# y_pred = predicted.cpu().data.numpy()\n",
    "\n",
    "# sensitivity = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "# precision = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "# f1 = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "\n",
    "# print('\\n\\nTest Model:\\n' + '-' * 50)\n",
    "# print(f'Sensitivity: {sensitivity:.2%}')\n",
    "# print(f'Precision: {precision:.2%}')\n",
    "# print(f'F1 (harmonic avarage): {f1:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLoaderData(data):\n",
    "    \"\"\"\n",
    "    The data loaders output a dictionary with 3 keys\n",
    "    The first 2 keys hold single values for the ImageID and the actual tensor of the image\n",
    "    The last key holds the ground truth vector of the 12 lables\n",
    "    \"\"\" \n",
    "    \n",
    "    ids, inputs, labels = data['id'], data['img'], data['labels']\n",
    "    # move data to device GPU OR CPU\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    return ids, inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictionsFromOutput(outputs):\n",
    "    \"\"\"\n",
    "    We are using BCEWithLogitsLoss for out loss\n",
    "    In this loss funciton, each label gets the sigmoid (inverse of Logit) before the CE loss\n",
    "    So our model outputs the raw values on the last FC layer\n",
    "    This means we have to apply sigmoid to our outputs to squash them between 0 and 1\n",
    "    We then take values >= .5 as Positive and < .5 as Negative \n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = torch.sigmoid(outputs.data) \n",
    "    predictions[predictions >= 0.5] = 1 # assign 1 label to those with less than 0.5\n",
    "    predictions[predictions < 0.5] = 0 # assign 0 label to those with less than 0.5   \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatePredictions(dictionary, ids, predictions):\n",
    "    \"\"\"\n",
    "    Keep track of predictions using the same index as our DataFrame\n",
    "    This will allow us to compare to the actual labels\n",
    "    \n",
    "    We only are taking the last prediction for each x-ray, but we could extend this later if wanted.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        id = ids[i].item()    \n",
    "        dictionary[id] = [int(f.item()) for f in predictions[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processBatch(net, data, optimizer=None):\n",
    "    \"\"\"\n",
    "    Used for both training and validation.\n",
    "    Validation will not pass in the optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert output from loader\n",
    "    ids, inputs, labels = parseLoaderData(data)\n",
    "    \n",
    "    if optimizer:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # Convert output to predicitons\n",
    "    outputs = net(inputs)\n",
    "    predictions = getPredictionsFromOutput(outputs)\n",
    "    \n",
    "    return ids, inputs, labels, outputs, predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(criterion, outputs, labels, optimizer):\n",
    "    \"\"\"\n",
    "    Get loss value from criterion\n",
    "    run backprop on the loss\n",
    "    update weights in optimizer\n",
    "    update epoch loss\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = criterion(outputs, labels)#.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictionDataFrame(epoch_predictions):\n",
    "    result = pd.DataFrame(epoch_predictions).transpose()\n",
    "    result.columns = target_columns\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = 0\n",
    "losses_hx = {}\n",
    "\n",
    "train_prediction_hx = {}\n",
    "val_prediction_hx = {}\n",
    "\n",
    "epoch_train_predictions = {}\n",
    "epoch_val_predictions = {}\n",
    "\n",
    "df_train_prediction = None\n",
    "df_val_prediction = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closeTrainEpoch(i):\n",
    "    global training_time_elapsed\n",
    "    global epoch_loss\n",
    "    global losses_hx\n",
    "    global train_prediction_hx\n",
    "    global last_train_predictions\n",
    "    global df_train_prediction\n",
    "    \n",
    "    training_time_elapsed = datetime.now() - start_time\n",
    "    epoch_loss = epoch_loss / len(train_loader)\n",
    "    losses_hx[i] = epoch_loss    \n",
    "    \n",
    "    df_train_prediction = getPredictionDataFrame(epoch_train_predictions)\n",
    "    train_prediction_hx[i] = df_train_prediction\n",
    "    last_train_predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closeValEpoch(i):\n",
    "    global validation_time_elapsed\n",
    "    global val_prediction_hx\n",
    "    global last_val_predictions\n",
    "    global df_val_prediction\n",
    "    \n",
    "    validation_time_elapsed = datetime.now() - start_time\n",
    "    \n",
    "    df_val_prediction = getPredictionDataFrame(epoch_val_predictions)\n",
    "    val_prediction_hx[i] = df_val_prediction\n",
    "    last_val_predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_epochs = 2\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)#, weight_decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], \n",
      "          Epoch Loss: 0.9844 \n",
      "          Training Time: 0:00:03.043863)  \n",
      "          Validation Time: 0:00:03.589405)\n",
      "Epoch [2/2], \n",
      "          Epoch Loss: 0.8000 \n",
      "          Training Time: 0:00:03.036882)  \n",
      "          Validation Time: 0:00:03.583421)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        ids, inputs, labels, outputs, predictions = processBatch(net, data, optimizer)\n",
    "        updatePredictions(epoch_train_predictions, ids, predictions)\n",
    "        epoch_loss += backProp(criterion, outputs, labels, optimizer)\n",
    "\n",
    "    closeTrainEpoch(i)\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "      for data in val_loader:          \n",
    "            ids, inputs, labels, _, predictions = processBatch(net, data)\n",
    "            updatePredictions(epoch_val_predictions, ids, predictions)\n",
    "   \n",
    "    closeValEpoch(i)\n",
    "    \n",
    "    \n",
    "    # stdout Results\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
    "\\n          Epoch Loss: {epoch_loss:.4f} \\\n",
    "\\n          Training Time: {training_time_elapsed})  \\\n",
    "\\n          Validation Time: {validation_time_elapsed})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImageResults(actual, predicted, imageID):\n",
    "    actual = actual[target_columns].transpose()\n",
    "    predicted = predicted.transpose()\n",
    "    result = pd.DataFrame()\n",
    "    result['Actual'] = actual[imageID]\n",
    "    result[result['Actual']==-1] = 0\n",
    "    result['Predicted'] = predicted[imageID]\n",
    "    result['Successful'] = result['Actual'] == result['Predicted']\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Successful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Enlarged_Cardiomediastinum</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Lung_Opacity</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Lung_Lesion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Edema</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Consolidation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Atelectasis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Pleural_Effusion</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Pleural_Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Fracture</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Actual  Predicted  Successful\n",
       "Enlarged_Cardiomediastinum       0          0        True\n",
       "Cardiomegaly                     0          0        True\n",
       "Lung_Opacity                     0          0        True\n",
       "Lung_Lesion                      0          0        True\n",
       "Edema                            0          0        True\n",
       "Consolidation                    0          0        True\n",
       "Pneumonia                        0          0        True\n",
       "Atelectasis                      0          1       False\n",
       "Pneumothorax                     0          0        True\n",
       "Pleural_Effusion                 1          1        True\n",
       "Pleural_Other                    0          0        True\n",
       "Fracture                         0          0        True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayImageResults(train_actual, \n",
    "                    df_train_prediction, \n",
    "                    45510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_result = train_actual.join(df_train_prediction, lsuffix='_actual', rsuffix='_predicted')\n",
    "df_val_result = val_actual.join(df_val_prediction, lsuffix='_actual', rsuffix='_predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>StudyID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex_Male</th>\n",
       "      <th>Sex_Unknown</th>\n",
       "      <th>Orientation_PA</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>Image_Path</th>\n",
       "      <th>Hierarchical_Path</th>\n",
       "      <th>Enlarged_Cardiomediastinum_actual</th>\n",
       "      <th>...</th>\n",
       "      <th>Lung_Opacity_predicted</th>\n",
       "      <th>Lung_Lesion_predicted</th>\n",
       "      <th>Edema_predicted</th>\n",
       "      <th>Consolidation_predicted</th>\n",
       "      <th>Pneumonia_predicted</th>\n",
       "      <th>Atelectasis_predicted</th>\n",
       "      <th>Pneumothorax_predicted</th>\n",
       "      <th>Pleural_Effusion_predicted</th>\n",
       "      <th>Pleural_Other_predicted</th>\n",
       "      <th>Fracture_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ImageID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>79521</td>\n",
       "      <td>19098</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data/raw/train/patient19098/study1/view1_front...</td>\n",
       "      <td>data/d21/d48/i79521.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8401</td>\n",
       "      <td>2079</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data/raw/train/patient02079/study1/view2_front...</td>\n",
       "      <td>data/d1/d29/i8401.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77894</td>\n",
       "      <td>18714</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data/raw/train/patient18714/study4/view1_front...</td>\n",
       "      <td>data/d44/d14/i77894.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12169</td>\n",
       "      <td>3031</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>data/raw/train/patient03031/study1/view1_front...</td>\n",
       "      <td>data/d19/d31/i12169.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187761</td>\n",
       "      <td>44734</td>\n",
       "      <td>3</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>data/raw/train/patient44734/study3/view1_front...</td>\n",
       "      <td>data/d11/d34/i187761.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191877</td>\n",
       "      <td>46242</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>data/raw/train/patient46242/study3/view1_front...</td>\n",
       "      <td>data/d27/d42/i191877.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223323</td>\n",
       "      <td>64462</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>data/raw/train/patient64462/study1/view1_front...</td>\n",
       "      <td>data/d23/d12/i223323.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136247</td>\n",
       "      <td>32694</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>data/raw/train/patient32694/study2/view1_front...</td>\n",
       "      <td>data/d47/d44/i136247.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42903</td>\n",
       "      <td>10503</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>data/raw/train/patient10503/study1/view1_front...</td>\n",
       "      <td>data/d3/d3/i42903.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210034</td>\n",
       "      <td>55119</td>\n",
       "      <td>3</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>data/raw/train/patient55119/study3/view1_front...</td>\n",
       "      <td>data/d34/d19/i210034.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PatientID  StudyID  Age  Sex_Male  Sex_Unknown  Orientation_PA  \\\n",
       "ImageID                                                                   \n",
       "79521        19098        1   68         0            0               1   \n",
       "8401          2079        1   71         0            0               1   \n",
       "77894        18714        4   58         1            0               0   \n",
       "12169         3031        1   59         1            0               1   \n",
       "187761       44734        3   71         0            0               0   \n",
       "...            ...      ...  ...       ...          ...             ...   \n",
       "191877       46242        3   66         0            0               0   \n",
       "223323       64462        1   49         0            0               0   \n",
       "136247       32694        2   51         0            0               0   \n",
       "42903        10503        1   57         1            0               1   \n",
       "210034       55119        3   71         1            0               0   \n",
       "\n",
       "         Support Devices                                         Image_Path  \\\n",
       "ImageID                                                                       \n",
       "79521                0.0  data/raw/train/patient19098/study1/view1_front...   \n",
       "8401                 0.0  data/raw/train/patient02079/study1/view2_front...   \n",
       "77894                0.0  data/raw/train/patient18714/study4/view1_front...   \n",
       "12169                0.0  data/raw/train/patient03031/study1/view1_front...   \n",
       "187761               1.0  data/raw/train/patient44734/study3/view1_front...   \n",
       "...                  ...                                                ...   \n",
       "191877               1.0  data/raw/train/patient46242/study3/view1_front...   \n",
       "223323               1.0  data/raw/train/patient64462/study1/view1_front...   \n",
       "136247               1.0  data/raw/train/patient32694/study2/view1_front...   \n",
       "42903                1.0  data/raw/train/patient10503/study1/view1_front...   \n",
       "210034               1.0  data/raw/train/patient55119/study3/view1_front...   \n",
       "\n",
       "                Hierarchical_Path  Enlarged_Cardiomediastinum_actual  ...  \\\n",
       "ImageID                                                               ...   \n",
       "79521     data/d21/d48/i79521.jpg                                  0  ...   \n",
       "8401        data/d1/d29/i8401.jpg                                  0  ...   \n",
       "77894     data/d44/d14/i77894.jpg                                  0  ...   \n",
       "12169     data/d19/d31/i12169.jpg                                  0  ...   \n",
       "187761   data/d11/d34/i187761.jpg                                  0  ...   \n",
       "...                           ...                                ...  ...   \n",
       "191877   data/d27/d42/i191877.jpg                                  0  ...   \n",
       "223323   data/d23/d12/i223323.jpg                                  0  ...   \n",
       "136247   data/d47/d44/i136247.jpg                                  0  ...   \n",
       "42903       data/d3/d3/i42903.jpg                                 -1  ...   \n",
       "210034   data/d34/d19/i210034.jpg                                  0  ...   \n",
       "\n",
       "         Lung_Opacity_predicted  Lung_Lesion_predicted  Edema_predicted  \\\n",
       "ImageID                                                                   \n",
       "79521                         1                      0                0   \n",
       "8401                          0                      1                1   \n",
       "77894                         0                      1                0   \n",
       "12169                         0                      0                0   \n",
       "187761                        0                      0                0   \n",
       "...                         ...                    ...              ...   \n",
       "191877                        0                      0                1   \n",
       "223323                        1                      0                1   \n",
       "136247                        0                      0                0   \n",
       "42903                         0                      0                0   \n",
       "210034                        1                      0                1   \n",
       "\n",
       "         Consolidation_predicted  Pneumonia_predicted  Atelectasis_predicted  \\\n",
       "ImageID                                                                        \n",
       "79521                          0                    0                      1   \n",
       "8401                           1                    1                      1   \n",
       "77894                          1                    1                      1   \n",
       "12169                          0                    0                      0   \n",
       "187761                         0                    0                      1   \n",
       "...                          ...                  ...                    ...   \n",
       "191877                         0                    0                      0   \n",
       "223323                         0                    0                      0   \n",
       "136247                         0                    0                      0   \n",
       "42903                          0                    0                      1   \n",
       "210034                         0                    0                      0   \n",
       "\n",
       "         Pneumothorax_predicted  Pleural_Effusion_predicted  \\\n",
       "ImageID                                                       \n",
       "79521                         0                           1   \n",
       "8401                          0                           1   \n",
       "77894                         0                           0   \n",
       "12169                         0                           0   \n",
       "187761                        0                           0   \n",
       "...                         ...                         ...   \n",
       "191877                        0                           0   \n",
       "223323                        0                           1   \n",
       "136247                        0                           0   \n",
       "42903                         0                           1   \n",
       "210034                        0                           1   \n",
       "\n",
       "         Pleural_Other_predicted  Fracture_predicted  \n",
       "ImageID                                               \n",
       "79521                          1                   1  \n",
       "8401                           0                   1  \n",
       "77894                          0                   0  \n",
       "12169                          0                   1  \n",
       "187761                         1                   1  \n",
       "...                          ...                 ...  \n",
       "191877                         0                   0  \n",
       "223323                         0                   0  \n",
       "136247                         0                   1  \n",
       "42903                          0                   1  \n",
       "210034                         0                   0  \n",
       "\n",
       "[158 rows x 33 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_result \n",
    "df_val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
