{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Dir:  C:\\git\\Springboard-Public\\Capstone Project 2\\notebooks\\Support Notebooks for Modules\n",
      "Root Path:  C:/git/Springboard-Public/Capstone Project 2/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os, os.path\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd() ,'/modules'))\n",
    "root_path = \"C:/git/Springboard-Public/Capstone Project 2/\"\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_path = \"/content/drive/My Drive/Capstone Project 2/\"\n",
    "\n",
    "print('Current Working Dir: ', os.getcwd())\n",
    "print('Root Path: ', root_path)\n",
    "\n",
    "# We need to set the working directory since we are using relative paths from various locations\n",
    "if os.getcwd() != root_path:\n",
    "  os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from modules.lib.ChextXRayImages import *\n",
    "from modules.models.CustomPneumonia import CustomPneumoniaNN\n",
    "\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on device=cuda\n"
     ]
    }
   ],
   "source": [
    "force_cpu = True\n",
    "device = torch.device('cuda' if ~force_cpu and torch.cuda.is_available() else 'cpu')\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(f'Working on device={device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Batches: 10\n",
      "Number of Validation Batches: 53\n",
      "Number of Training Images: 160\n",
      "Number of Validation Images: 848\n"
     ]
    }
   ],
   "source": [
    "loaders = Loaders()\n",
    "batch_size=16\n",
    "val_percent=0.15\n",
    "number_images = 1000\n",
    "train_loader, val_loader = loaders.getDataTrainValidateLoaders(batch_size=batch_size, \n",
    "                                                                        val_percent=val_percent, \n",
    "                                                                        n_random_rows=number_images)\n",
    "print(f'Number of Training Batches: {len(train_loader):,}')\n",
    "print(f'Number of Validation Batches: {len(val_loader):,}')\n",
    "print(f'Number of Training Images: {len(train_loader) * batch_size:,}')\n",
    "print(f'Number of Validation Images: {len(val_loader) * batch_size:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)       \n",
    "        self.flattened_length_ = 1*320*320\n",
    "        self.fc1 = nn.Linear(self.flattened_length_, 12)\n",
    "       \n",
    "    def forward(self, x):    \n",
    "        x = x.view(-1, self.flattened_length_)    \n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 12]       1,228,812\n",
      "       SimpleModel-2                   [-1, 12]               0\n",
      "================================================================\n",
      "Total params: 1,228,812\n",
      "Trainable params: 1,228,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 4.69\n",
      "Estimated Total Size (MB): 5.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = SimpleModel()\n",
    "\n",
    "net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "\n",
    "summary(net, (1, 320, 320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)#, weight_decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 12])\n",
      "torch.Size([16, 1, 320, 320])\n",
      "torch.Size([16, 12])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "inputs, labels = data['img'], data['labels']\n",
    "\n",
    "print(labels.shape)\n",
    "\n",
    "# move data to device GPU OR CPU\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = net(inputs)\n",
    "print(inputs.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    # get the inputs\n",
    "    inputs, labels = data['img'], data['labels']\n",
    "    \n",
    "    # move data to device GPU OR CPU\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    #loss, back prop and update params\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],           Epoch Loss: 0.7111           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.450771)\n",
      "Epoch [2/10],           Epoch Loss: 0.7365           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.447802)\n",
      "Epoch [3/10],           Epoch Loss: 0.6568           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.446804)\n",
      "Epoch [4/10],           Epoch Loss: 0.7316           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.446804)\n",
      "Epoch [5/10],           Epoch Loss: 0.6854           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.447801)\n",
      "Epoch [6/10],           Epoch Loss: 0.8216           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.446804)\n",
      "Epoch [7/10],           Epoch Loss: 0.7843           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.458773)\n",
      "Epoch [8/10],           Epoch Loss: 0.7101           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.440820)\n",
      "Epoch [9/10],           Epoch Loss: 0.7233           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.456778)\n",
      "Epoch [10/10],           Epoch Loss: 0.6858           Training Accuracy: 0.0000            Validation Accuracy: 0.0000 - (time=0:00:00.457776)\n"
     ]
    }
   ],
   "source": [
    "train_accuracy_index = []\n",
    "train_acc, train_total, train_correct = 0, 0, 0\n",
    "val_accuracy_index = []\n",
    "val_acc, val_total, val_correct = 0, 0, 0\n",
    "test_accuracy_index = []\n",
    "test_acc, test_total, test_correct = 0, 0, 0\n",
    "losses_index = []\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    start_time = datetime.now()\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data['img'], data['labels']\n",
    "        # move data to device GPU OR CPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        #Get Training Accuracty\n",
    "        # We are using BCEWithLogitsLoss for out loss\n",
    "        # In this loss funciton, each label gets the sigmoid (inverse of Logit) before the CE loss\n",
    "        # So our model outputs the raw values on the last FC layer\n",
    "        # This means we have to apply sigmoid to our outputs to squash them between 0 and 1\n",
    "        # We then take values >= .5 as Positive and < .5 as Negative\n",
    "        predicted = outputs.data\n",
    "        predicted = torch.sigmoid(predicted) \n",
    "        predicted[predicted >= 0.5] = 1 # assign 1 label to those with less than 0.5\n",
    "        predicted[predicted < 0.5] = 0 # assign 0 label to those with less than 0.5\n",
    "        train_batch_size, train_label_count = labels.shape\n",
    "        train_acc = (predicted == labels).sum() / (train_batch_size * train_label_count)\n",
    "        train_accuracy_index.append(train_acc)\n",
    "\n",
    "        #loss, back prop and update params\n",
    "        loss = criterion(outputs, labels)#.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss / len(train_loader)\n",
    "    time_elapsed = datetime.now() - start_time\n",
    "    losses_index.append(epoch_loss)\n",
    "    \n",
    "    # Validation set\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "      for data in val_loader:\n",
    "          inputs, labels = data['img'], data['labels']\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          outputs = net(inputs)\n",
    "          predicted = outputs.data\n",
    "          predicted = torch.sigmoid(predicted) \n",
    "          predicted[predicted >= 0.5] = 1 \n",
    "          predicted[predicted < 0.5] = 0 \n",
    "          val_batch_size, val_label_count = labels.shape\n",
    "          val_acc = (predicted == labels).sum() / (val_batch_size * val_label_count)\n",
    "          val_accuracy_index.append(val_acc)\n",
    "   \n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
    "          Epoch Loss: {epoch_loss:.4f} \\\n",
    "          Training Accuracy: {train_acc:.4f}  \\\n",
    "          Validation Accuracy: {val_acc:.4f} - (time={time_elapsed})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
