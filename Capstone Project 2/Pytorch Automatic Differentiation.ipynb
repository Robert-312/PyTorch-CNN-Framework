{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd package in PyTorch\n",
    "\n",
    "The goal of this notebook is to play around with this package to get a better understanding of how PyTorch does automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "We need to wrap our tensors as variables to be able to generate the back propagation and our gradients.\n",
    "\n",
    "Here, we create a simple scaler value $x=4$.  Even though it is nondimensional, we still need to make it a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([4]), requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single variable polynomial\n",
    "\n",
    "## $z = x^{3} + 2x^{2} + 8x$\n",
    "\n",
    "We will use a lambda function for this polynominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "func_1 = lambda x: torch.pow(x, 3) + 2*torch.pow(x, 2) + 8*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Derivative\n",
    "\n",
    "Let's manually do the math to get the first derivative and evaluate it for $x=4$.\n",
    "\n",
    "### $\\frac{dz}{dx} = 3x^{2} + 4x + 8$\n",
    "\n",
    "### $\\frac{dz}{dx}\\Bigr|_{\\substack{x=4}} \\quad 3*4^{2} + 4*4 + 8 = 72$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see if PyTorch gets the same results\n",
    "\n",
    "Generate a new tensor $z$ that is the result of our function.\n",
    "\n",
    "We then take this result tensor and run the back propagation method.\n",
    "\n",
    "This sets the input variable $x$ grad attribute to first derivative evaluated at $x=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([72.])\n"
     ]
    }
   ],
   "source": [
    "z = func_1(x)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The computational graph (see below) is released automatically on the backward() call.  So running backward() twice throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    z.backward()\n",
    "except RuntimeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we tell PyTorch to ratain the computation graph, we can run backward() multiple times\n",
    "\n",
    "But all this does is add sum the gradients again.\n",
    "\n",
    "i.e. This does NOT do the second derivative \n",
    "\n",
    "72\n",
    "\n",
    "72+72=144\n",
    "\n",
    "72+72+72=216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([72.])\n",
      "tensor([144.])\n",
      "tensor([216.])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad and find z again\n",
    "x.grad.data.zero_()\n",
    "z = func_1(x)\n",
    "\n",
    "z.backward(retain_graph = True)\n",
    "print(x.grad)\n",
    "z.backward(retain_graph = True)\n",
    "print(x.grad)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph\n",
    "\n",
    "Let's write our polynomial as composite functions to show how it would look as a computational graph.\n",
    "\n",
    "There are 3 terms in the polynomial, so this will create 3 branches in our computational graph.\n",
    "\n",
    "## $z = x^{3} + 2x^{2} + 8x$\n",
    "\n",
    "### Branch A: $x^3$\n",
    "\n",
    "- $\\dot{A_1} = 3x^2 \\quad \\rightarrow \\quad \\dot{A_1}\\Bigr|_{\\substack{x=4}} = 48$\n",
    "\n",
    "### Branch B: $2x^2$\n",
    "\n",
    "- $\\dot{B_1} = 2x \\quad \\rightarrow \\quad \\dot{B_1}\\Bigr|_{\\substack{x=4}} = 8$\n",
    "\n",
    "\n",
    "- $\\dot{B_2} = 2\\dot{B_1} \\quad \\rightarrow \\quad \\dot{B_2}\\Bigr|_{\\substack{x=4}} = 2 \\times 8 = 16$ *(chain rule)*\n",
    "\n",
    "### Branch C: $8x$\n",
    "\n",
    "- $\\dot{C_1} = 8 \\quad \\rightarrow \\quad \\dot{C_1}\\Bigr|_{\\substack{x=4}} = 8$\n",
    "\n",
    "### Final Gradient for x=4:\n",
    "- $\\dot{A_1} + \\dot{B_2} + \\dot{C_1} = 48 + 16 + 8 = 72$\n",
    "\n",
    "## This is how the gradient for the forward propagation is stored.  \n",
    "\n",
    "PyTorch does not use calculus per se, it has a set of known derivative equations for most math functions.\n",
    "\n",
    "During the forward, these local gradient values are stored for each branch and each step in that branch.\n",
    "\n",
    "On back propagation, the chain rule is used.  For each branch, basically all that needs to be done is to multiple all the local gradients together to the the final gradient, which makes for a very efficient process since only basic math operators are used.\n",
    "\n",
    "From PyTorch's perspective, the backwards() call boils down to a series of matrix multiplications.\n",
    "\n",
    "#### Last Local Gradient\n",
    "\n",
    "You can think of the final local gradiant as $\\frac{\\partial{z}}{\\partial{z}}$ which is always equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's look at a single branch function with sevaral steps\n",
    "\n",
    "This will give us a better look at how a branch stores the local gradients.\n",
    "\n",
    "### $z = i \\circ h \\circ g \\circ f $\n",
    "\n",
    "- $f(x) = 2x$\n",
    "- $g(f) = f^4$\n",
    "- $h(g) = {3.14}g$\n",
    "- $i(h) = \\sqrt[3]{h}$\n",
    "- $z=i$\n",
    "\n",
    "### Let's look the derivatives for each step:\n",
    "- $\\frac{\\partial{f}}{\\partial{x}} = 2$\n",
    "- $\\frac{\\partial{g}}{\\partial{f}} = 4f^3$\n",
    "- $\\frac{\\partial{h}}{\\partial{g}} = 3.14$\n",
    "- $\\frac{\\partial{i}}{\\partial{h}} = \\frac{1}{2\\sqrt{h}}$\n",
    "- $\\frac{\\partial{z}}{\\partial{i}} = 1$\n",
    "\n",
    "### Now evaluate these with $x=4$:\n",
    "- $f(4) = 8$\n",
    "- $g(f) = 8^4 = 4096$\n",
    "- $h(g) = 3.14 \\times 4096 = 12861.44$\n",
    "- $i(h) = \\sqrt{12861.44} = 113.4083$\n",
    "\n",
    "\n",
    "- $\\frac{\\partial{f}}{\\partial{x}} = 2$\n",
    "- $\\frac{\\partial{g}}{\\partial{f}} = 4f^3 = 4 \\times 8^3 = 2048$\n",
    "- $\\frac{\\partial{h}}{\\partial{g}} = 3.14$\n",
    "- $\\frac{\\partial{i}}{\\partial{h}} = \\frac{1}{2\\sqrt{h}} = \\frac{1}{2\\sqrt{12861.44}} = 0.004408849$\n",
    "- $\\frac{\\partial{z}}{\\partial{i}} = 1$\n",
    "\n",
    "### Now we get the backwards local gradients via the chain rule (simple multiplication):\n",
    "- $\\frac{\\partial{z}}{\\partial{i}} = 1$\n",
    "- $\\frac{\\partial{z}}{\\partial{h}} = \\frac{\\partial{z}}{\\partial{i}} \\cdot \\frac{\\partial{i}}{\\partial{h}}$\n",
    "- $\\frac{\\partial{z}}{\\partial{g}} = \\frac{\\partial{z}}{\\partial{i}} \\cdot \\frac{\\partial{i}}{\\partial{h}}  \\cdot \\frac{\\partial{h}}{\\partial{g}}$\n",
    "- $\\frac{\\partial{z}}{\\partial{f}} = \\frac{\\partial{z}}{\\partial{i}} \\cdot \\frac{\\partial{i}}{\\partial{h}}  \\cdot \\frac{\\partial{h}}{\\partial{g}} \\cdot \\frac{\\partial{g}}{\\partial{f}}$\n",
    "- $\\frac{\\partial{z}}{\\partial{x}} = \\frac{\\partial{z}}{\\partial{i}} \\cdot \\frac{\\partial{i}}{\\partial{h}}  \\cdot \\frac{\\partial{h}}{\\partial{g}} \\cdot \\frac{\\partial{g}}{\\partial{f}} \\cdot \\frac{\\partial{f}}{\\partial{x}}$\n",
    "\n",
    "### The local gradients evaluated with $x=4$:\n",
    "\n",
    "- $\\frac{\\partial{z}}{\\partial{i}} = \\textbf{1}$\n",
    "\n",
    "\n",
    "- $\\frac{\\partial{z}}{\\partial{h}} = 1 \\times 0.004408849 = \\textbf{0.004408849}$\n",
    "\n",
    "\n",
    "- $\\frac{\\partial{z}}{\\partial{g}} = 0.004408849 \\times 3.14 = \\textbf{0.01384378586}$\n",
    "\n",
    "\n",
    "- $\\frac{\\partial{z}}{\\partial{f}} = 0.01384378586 \\times 2048 = \\textbf{28.35207344128}$\n",
    "\n",
    "\n",
    "- $\\frac{\\partial{z}}{\\partial{x}} = 28.35207344128 \\times 2 = \\textbf{56.70414688256}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of composit functions (x=4):\n",
      "z = i = 113.40828704833984\n",
      "\n",
      "Local Gradients:\n",
      "dz/di:  1.0\n",
      "dz/dh:  0.004408848937600851\n",
      "dz/dg:  0.013843785971403122\n",
      "dz/df:  28.352073669433594\n",
      "\n",
      "Final Gradient (x=4)\n",
      "dz/dx:  56.70414733886719\n"
     ]
    }
   ],
   "source": [
    "comp_f = lambda x: 2 * x\n",
    "comp_g = lambda f: torch.pow(f, 4)\n",
    "comp_h = lambda g: 3.14 * g\n",
    "comp_i = lambda h: torch.sqrt(h)\n",
    "\n",
    "x = Variable(torch.FloatTensor([4]), requires_grad=True)\n",
    "f = comp_f(x); g = comp_g(f); h = comp_h(g); i = comp_i(h); z=i\n",
    "\n",
    "print('Result of composit functions (x=4):\\nz = i =', i.item())\n",
    "\n",
    "# By default, PyTorch disposes the local gradients on backprop except for the leaf level\n",
    "# But we can put in a hook to show what these interim local gradient values are\n",
    "printf = lambda f: print('dz/df: ', f.item()); printg = lambda g: print('dz/dg: ', g.item())\n",
    "printh = lambda h: print('dz/dh: ', h.item()); printi = lambda i: print('dz/di: ', i.item())\n",
    "print(\"\\nLocal Gradients:\")\n",
    "f.register_hook(printf); g.register_hook(printg); h.register_hook(printh); i.register_hook(printi)\n",
    "\n",
    "z.backward()\n",
    "print('\\nFinal Gradient (x=4)\\ndz/dx: ', x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent of Loss Function\n",
    "\n",
    "#### Normally, we get the gradiant on a loss function.  If we want the loss to be less, we go in the oppisit direction of the gradient. \n",
    "\n",
    "$\\Delta x = -\\frac{\\partial{z}}{\\partial{x}}$\n",
    "\n",
    "We then repeat changing x by $\\Delta x$ to decrease the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 1\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  23\n",
      "Loss:  3749.5615234375\n",
      "dz/dx:  326.048828125\n",
      "\n",
      "\n",
      "\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 2\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  -9.604882812500001\n",
      "Loss:  653.896484375\n",
      "dz/dx:  -136.1591796875\n",
      "\n",
      "\n",
      "\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 3\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  4.011035156249999\n",
      "Loss:  114.03488159179688\n",
      "dz/dx:  56.86058044433594\n",
      "\n",
      "\n",
      "\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 4\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  -1.6750228881835945\n",
      "Loss:  19.886863708496094\n",
      "dz/dx:  -23.74518394470215\n",
      "\n",
      "\n",
      "\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 5\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  0.6994955062866204\n",
      "Loss:  3.4681243896484375\n",
      "dz/dx:  9.9160737991333\n",
      "\n",
      "\n",
      "\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 6\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  -0.29211187362670976\n",
      "Loss:  0.6048159599304199\n",
      "dz/dx:  -4.140988349914551\n",
      "\n",
      "\n",
      "\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 7\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  0.12198696136474535\n",
      "Loss:  0.105475515127182\n",
      "dz/dx:  1.7292916774749756\n",
      "\n",
      "\n",
      "\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 8\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  -0.05094220638275221\n",
      "Loss:  0.018394174054265022\n",
      "dz/dx:  -0.7221586108207703\n",
      "\n",
      "\n",
      "\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 9\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  0.021273654699324823\n",
      "Loss:  0.0032078127842396498\n",
      "dz/dx:  0.3015761077404022\n",
      "\n",
      "\n",
      "\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "Iteration: 10\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "New x:  -0.008883956074715399\n",
      "Loss:  0.0005594195099547505\n",
      "dz/dx:  -0.125939279794693\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_value = 23\n",
    "learning_rate = 1e-1\n",
    "for iteration in range(1, 11):\n",
    "    x = Variable(torch.FloatTensor([x_value]), requires_grad=True)\n",
    "    f = comp_f(x)\n",
    "    g = comp_g(f)\n",
    "    h = comp_h(g)\n",
    "    i = comp_i(h)\n",
    "    loss=i\n",
    "    \n",
    "    print(u\"\\u2585\" * 30)\n",
    "    print(f'Iteration: {iteration}')\n",
    "    print(u\"\\u2585\" * 30)\n",
    "\n",
    "    print('New x: ', x_value)\n",
    "    print(f'Loss: ', loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    print(f'dz/dx: ', x.grad.item())\n",
    "    \n",
    "    print('\\n' * 2)\n",
    "\n",
    "    x_value = x_value - (learning_rate * x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try this on a non-polynominal function\n",
    "\n",
    "$z = {sin({6x})}^2 \\times {2x}^{2} \\times {|x|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: 22.44702911376953\n",
      "dz/dx:  90.7747802734375\n"
     ]
    }
   ],
   "source": [
    "def non_poly(x):\n",
    "    return torch.pow(torch.sin(6*x), 2) * torch.pow(2*x, 2) * torch.abs(x)\n",
    "\n",
    "x = Variable(torch.FloatTensor([1.8]), requires_grad=True) \n",
    "z = non_poly(x)\n",
    "z.backward()\n",
    "print('z:', z.item())\n",
    "print(f'dz/dx: ', x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = torch.from_numpy(np.arange(0, 2, .01))\n",
    "Z = non_poly(X)\n",
    "plt.plot(X, Z, color='b')\n",
    "plt.plot(1.33974, 9.295228958129883, marker='o', color='r')\n",
    "plt.plot(1.85492, 25.0738, marker='o', color='r')\n",
    "plt.plot(np.pi / 2, 0, marker='o', color='r')\n",
    "plt.plot(1.8, 22.447, marker='x', color='g')\n",
    "plt.plot([1.55262, 1.83905], [0, 26], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see from the plot above, our non_poly function is anything but linear\n",
    "\n",
    "There are several local maximas and minimas with x between 0 and 2.\n",
    "\n",
    "The are local maximas around x = 1.33974 and x = 1.85492.\n",
    "\n",
    "There is a local minima at $\\frac{\\pi}{2} \\approx 1.570796$\n",
    "\n",
    "#### So, let's start our gradient descent at x = 1.8, just before a local maxima.\n",
    "\n",
    "The slope at this point is about 90.774.\n",
    "\n",
    "So we will adjust the value of x in the following way:\n",
    "- $\\Delta{x} = {-1} \\times (90.772 \\times learning_rate)$\n",
    "\n",
    "If the learning rate = 1e-1, this would give:\n",
    "- $\\Delta{x} = {-1} \\times (90.772 \\times .1) = -9.0772$\n",
    "- $x + \\Delta{x} = 1.8 - 9.0772 = -7.2772$\n",
    "\n",
    "But with this much change, we would move across multiple local minimas and not even be in this plot above.\n",
    "\n",
    "### Picking the right learning rate is critical:\n",
    "\n",
    "If the learning rate = 1e-3, this would give:\n",
    "- $\\Delta{x} = {-1} \\times (90.772 \\times .001) = -0.090772$\n",
    "- $x + \\Delta{x} = 1.8 - 0.090772 = 1.709228$\n",
    "\n",
    "This makes the change in x much smaller as keeps us between the 2 local minimas.\n",
    "\n",
    "*Note: We don't want just find the local minima, but the global minima.*\n",
    "*In this case, the local minimas happen to be zero so we would be ok.*\n",
    "*We will need to account for getting stuck in local minimas with things like stochastic gradient descent vs Adam optimizers, but this is out of scope for this notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run several iterations of gradient descent for out non_poly function with 3 different learning rates\n",
    "\n",
    "#### Our goal will be to get to:\n",
    "### $x = \\frac{\\pi}{2} \\approx 1.570796$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "LR=0.1\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "------------------------------\n",
      "Iteration: 1\n",
      "------------------------------\n",
      "x:  1.8\n",
      "Loss:  22.44702911376953\n",
      "dz/dx:  90.7747802734375\n",
      "------------------------------\n",
      "Iteration: 2\n",
      "------------------------------\n",
      "x:  -7.27747802734375\n",
      "Loss:  150.1937713623047\n",
      "dz/dx:  5424.029296875\n",
      "------------------------------\n",
      "Iteration: 3\n",
      "------------------------------\n",
      "x:  -549.6804077148437\n",
      "Loss:  205495120.0\n",
      "dz/dx:  3683689472.0\n",
      "------------------------------\n",
      "Iteration: 4\n",
      "------------------------------\n",
      "x:  -368369496.88040775\n",
      "Loss:  8.620856699699118e+23\n",
      "dz/dx:  1.572076355549627e+26\n",
      "------------------------------\n",
      "Iteration: 5\n",
      "------------------------------\n",
      "x:  -1.5720763555496272e+25\n",
      "Loss:  inf\n",
      "dz/dx:  nan\n",
      "------------------------------\n",
      "Iteration: 6\n",
      "------------------------------\n",
      "x:  nan\n",
      "Loss:  nan\n",
      "dz/dx:  nan\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "LR=0.01\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "------------------------------\n",
      "Iteration: 1\n",
      "------------------------------\n",
      "x:  1.8\n",
      "Loss:  22.44702911376953\n",
      "dz/dx:  90.7747802734375\n",
      "------------------------------\n",
      "Iteration: 2\n",
      "------------------------------\n",
      "x:  0.892252197265625\n",
      "Loss:  1.824934959411621\n",
      "dz/dx:  -10.207292556762695\n",
      "------------------------------\n",
      "Iteration: 3\n",
      "------------------------------\n",
      "x:  0.9943251228332519\n",
      "Loss:  0.3826386630535126\n",
      "dz/dx:  -12.830721855163574\n",
      "------------------------------\n",
      "Iteration: 4\n",
      "------------------------------\n",
      "x:  1.1226323413848878\n",
      "Loss:  1.0823249816894531\n",
      "dz/dx:  29.60115623474121\n",
      "------------------------------\n",
      "Iteration: 5\n",
      "------------------------------\n",
      "x:  0.8266207790374757\n",
      "Loss:  2.1239075660705566\n",
      "dz/dx:  1.2725753784179688\n",
      "------------------------------\n",
      "Iteration: 6\n",
      "------------------------------\n",
      "x:  0.8138950252532959\n",
      "Loss:  2.0941433906555176\n",
      "dz/dx:  3.3798866271972656\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "LR=0.001\n",
      "▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "------------------------------\n",
      "Iteration: 1\n",
      "------------------------------\n",
      "x:  1.8\n",
      "Loss:  22.44702911376953\n",
      "dz/dx:  90.7747802734375\n",
      "------------------------------\n",
      "Iteration: 2\n",
      "------------------------------\n",
      "x:  1.7092252197265625\n",
      "Loss:  10.88791561126709\n",
      "dz/dx:  138.46347045898438\n",
      "------------------------------\n",
      "Iteration: 3\n",
      "------------------------------\n",
      "x:  1.570761749267578\n",
      "Loss:  6.637591241087648e-07\n",
      "dz/dx:  -0.03849175199866295\n",
      "------------------------------\n",
      "Iteration: 4\n",
      "------------------------------\n",
      "x:  1.5708002410195767\n",
      "Loss:  8.13860712156611e-09\n",
      "dz/dx:  0.0042625474743545055\n",
      "------------------------------\n",
      "Iteration: 5\n",
      "------------------------------\n",
      "x:  1.5707959784721024\n",
      "Loss:  5.499839819678165e-11\n",
      "dz/dx:  -0.00035040138754993677\n",
      "------------------------------\n",
      "Iteration: 6\n",
      "------------------------------\n",
      "x:  1.5707963288734899\n",
      "Loss:  8.818357522877903e-15\n",
      "dz/dx:  4.436953986441949e-06\n"
     ]
    }
   ],
   "source": [
    "def gradientDescentNonPoly(initial_x, iterations, learning_rate):\n",
    "    print(u\"\\u2585\" * 30)\n",
    "    print(f'LR={learning_rate}')\n",
    "    print(u\"\\u2585\" * 30)\n",
    "    x_value = initial_x   \n",
    "    for iteration in range(1, iterations+1):\n",
    "        x = Variable(torch.FloatTensor([x_value]), requires_grad=True)\n",
    "        loss=non_poly(x)\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(f'Iteration: {iteration}')\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        print('x: ', x_value)\n",
    "        print(f'Loss: ', loss.item())\n",
    "        print(f'dz/dx: ', x.grad.item())\n",
    "\n",
    "        x_value = x_value - (learning_rate * x.grad.item())\n",
    "        \n",
    "        \n",
    "gradientDescentNonPoly(initial_x = 1.8, iterations=6, learning_rate=1e-1)    \n",
    "gradientDescentNonPoly(initial_x = 1.8, iterations=6, learning_rate=1e-2) \n",
    "gradientDescentNonPoly(initial_x = 1.8, iterations=6, learning_rate=1e-3)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We were able to get to our goad with the learning rate of 1e-3\n",
    "\n",
    "After 6 iterations, we got to x=1.570796 which is our expected goal (with 7 sig figs) and got our loss function to almost zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions with multiple variables\n",
    "\n",
    "So far, we have only looked at functions with a single variable x.\n",
    "\n",
    "Let look to see what happens with a function with 2 variables x and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynominal with 2 variables\n",
    "\n",
    "### $z = x^{3} + y^{2} + x + 6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial z}{\\partial x} = 3x^{2} + 1$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}\\Bigr|_{\\substack{x=4}} \\quad 3*4^{2} + 1 = 49$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y} = 2y$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial y}\\Bigr|_{\\substack{y=2}} \\quad 2*2 = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[x, y] = [4. 2.]\n",
      "\n",
      "Value of Z:  78.0 \n",
      "\n",
      "[dz/dx, dz/dy] = [49.  4.]\n"
     ]
    }
   ],
   "source": [
    "x=4; y=2\n",
    "XY = Variable(torch.FloatTensor([x, y]), requires_grad=True)\n",
    "\n",
    "print(f'[x, y] = {XY.data.numpy()}')\n",
    "\n",
    "def elip(XY):\n",
    "    X = XY[0]\n",
    "    Y = XY[1]\n",
    "    return torch.pow(X, 3) + torch.pow(Y, 2) + X + 6\n",
    "Z = elip(XY)\n",
    "\n",
    "print('\\nValue of Z: ', Z.item(), '\\n')\n",
    "\n",
    "Z.backward()\n",
    "print(f'[dz/dx, dz/dy] = {XY.grad.data.numpy()}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is not much different here other then our function took in 2 variables\n",
    "\n",
    "Since the input was 2 different variables, the gradient returns 2 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's take our 2 variable function and supply it with multiple rows\n",
    "\n",
    "Think of each row as a new sample:\n",
    "- Person A: x=4, y=2\n",
    "- Person B: x=3, y=2\n",
    "- Person C: x=4, y=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person A: [x, y] = [4. 2.]\n",
      "Person B: [x, y] = [3. 2.]\n",
      "Person C: [x, y] = [4. 3.]\n",
      "\n",
      "Person A: z = 78.0\n",
      "Person B: z = 40.0\n",
      "Person C: z = 83.0\n",
      "\n",
      "Person A: [dz/dx, dz/dy] = [49.  4.]\n",
      "Person B: [dz/dx, dz/dy] = [28.  4.]\n",
      "Person C: [dz/dx, dz/dy] = [49.  6.]\n"
     ]
    }
   ],
   "source": [
    "XY = Variable(torch.FloatTensor([[4, 2],\n",
    "                                 [3, 2],\n",
    "                                 [4, 3]]), requires_grad=True)\n",
    "\n",
    "print(f'Person A: [x, y] = {XY[0].data.numpy()}')\n",
    "print(f'Person B: [x, y] = {XY[1].data.numpy()}')\n",
    "print(f'Person C: [x, y] = {XY[2].data.numpy()}\\n')                                 \n",
    "\n",
    "def multi_row_elip(XY):\n",
    "    X = XY[:,0]\n",
    "    Y = XY[:,1]\n",
    "    return torch.pow(X, 3) + torch.pow(Y, 2) + X + 6\n",
    "Z = multi_row_elip(XY)\n",
    "\n",
    "print(f'Person A: z = {Z[0].item()}')\n",
    "print(f'Person B: z = {Z[1].item()}')\n",
    "print(f'Person C: z = {Z[2].item()}\\n')  \n",
    "\n",
    "row_count = XY.shape[0]\n",
    "vector_of_ones = torch.ones(row_count)\n",
    "Z.backward(vector_of_ones) #Explained below\n",
    "\n",
    "print(f'Person A: [dz/dx, dz/dy] = {XY.grad.data[0].numpy()}') \n",
    "print(f'Person B: [dz/dx, dz/dy] = {XY.grad.data[1].numpy()}') \n",
    "print(f'Person C: [dz/dx, dz/dy] = {XY.grad.data[2].numpy()}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's look at this using the Iris Dataset\n",
    "\n",
    "This should give us a better feel for how this applies to neural networks.\n",
    "\n",
    "We will use a simple function for Z now, but will replace this will an a function will a real loss function down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Names:  ['setosa' 'versicolor' 'virginica'] \n",
      "\n",
      "Row Count:  150\n",
      "Feature Count:  4\n",
      "Feature Names:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] \n",
      "\n",
      "Input Data:\n",
      "sample of setosa: data = [5.0, 3.0, 1.6, 0.2]\n",
      "sample of versicolor: data = [6.6, 3.0, 4.4, 1.4]\n",
      "sample of virginica: data = [7.2, 3.2, 6.0, 1.8]\n",
      "\n",
      "Function Results:\n",
      "sample of setosa: z = 15.319999694824219\n",
      "sample of versicolor: z = 25.959999084472656\n",
      "sample of virginica: z = 33.839996337890625\n",
      "\n",
      "Gradients:\n",
      "sample of setosa: [dz/d(sl), dz/d(sw), dz/d(pl), dz/d(pw)] = [3.0, 5.0, 0.2, 1.6]\n",
      "sample of versicolor: [dz/d(sl), dz/d(sw), dz/d(pl), dz/d(pw)] = [3.0, 6.6, 1.4, 4.4]\n",
      "sample of virginica: [dz/d(sl), dz/d(sw), dz/d(pl), dz/d(pw)] = [3.2, 7.2, 1.8, 6.0]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris_samples = iris.data\n",
    "iris_targets = iris.target\n",
    "\n",
    "\n",
    "print('Target Names: ', iris.target_names, '\\n')\n",
    "row_count = iris_samples.shape[0]\n",
    "print('Row Count: ', row_count)\n",
    "print('Feature Count: ', iris_samples.shape[1])\n",
    "print('Feature Names: ', iris.feature_names, '\\n')\n",
    "\n",
    "# a single row (or sample) from the 3 species can be seen with these indexes: 25, 75, 125\n",
    "\n",
    "iris_samples_tensor = Variable(torch.FloatTensor(iris_samples), requires_grad=True)\n",
    "\n",
    "print('Input Data:')\n",
    "print(f'sample of setosa: data = {list(iris_samples_tensor[25].data.numpy())}')\n",
    "print(f'sample of versicolor: data = {list(iris_samples_tensor[75].data.numpy())}')\n",
    "print(f'sample of virginica: data = {list(iris_samples_tensor[125].data.numpy())}\\n')  \n",
    "\n",
    "def iris_surface_area_function(iris_samples):\n",
    "    sl = iris_samples[:,0]\n",
    "    sw = iris_samples[:,1]\n",
    "    pl = iris_samples[:,2]\n",
    "    pw = iris_samples[:,3]\n",
    "    return sl * sw + pl * pw # Sort of area of sepal + area of petal\n",
    "\n",
    "Z = iris_surface_area_function(iris_samples_tensor)\n",
    "\n",
    "print('Function Results:')\n",
    "print(f'sample of setosa: z = {Z[25].item()}')\n",
    "print(f'sample of versicolor: z = {Z[75].item()}')\n",
    "print(f'sample of virginica: z = {Z[125].item()}\\n')  \n",
    "\n",
    "vector_of_ones = torch.ones(row_count)\n",
    "Z.backward(vector_of_ones) #Explained below\n",
    "\n",
    "print('Gradients:')\n",
    "grad_string = '[dz/d(sl), dz/d(sw), dz/d(pl), dz/d(pw)]' #with repect to sepal length/width petal length/width\n",
    "print(f'sample of setosa: {grad_string} = {list(iris_samples_tensor.grad.data[25].numpy())}') \n",
    "print(f'sample of versicolor: {grad_string} = {list(iris_samples_tensor.grad.data[75].numpy())}') \n",
    "print(f'sample of virginica: {grad_string} = {list(iris_samples_tensor.grad.data[125].numpy())}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function is contrived, and probably is not a great way to predict the 3 species\n",
    "\n",
    "#### But the intention here is to show how a gradient works with multiple features and rows.\n",
    "\n",
    "We can see from the gradients, which features have a bigger influence on the function result Z.\n",
    "\n",
    "Across the 3 samples of the 3 species, you can see that the first 2 gradients are larger.  These are the gradients for sepal length and sepal width.  Of these 2 gradients, the sepal width has a larger gradient and therefore a larger influence on Z.  For Versicolor and Virginica, the last gradient (petal width) also has a pretty strong influence on Z.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector of Ones\n",
    "\n",
    "We need to explain what this is why we sometimes need to pass this into our backward() function call.\n",
    "\n",
    "This vector is passed for the gradient parameter of the backward() call.  \n",
    "\n",
    "It's \"default\" value is **torch.ones([1])**.\n",
    "\n",
    "You can think of the **1** value as the number of rows. \n",
    "\n",
    "Most loss functions reduce the output to a single scalar value.  Therefor, the number of \"rows\" returned by the function is **1**.\n",
    "\n",
    "But some loss functions can produce multiple rows.  An example of this would be **BCEWithLogitsLoss(reduction=None)**.\n",
    "\n",
    "If the function result we call returns **n** rows, we need to pass in **torch.ones([n])** into the backward() call.\n",
    "\n",
    "A good way of thinking about this is that each sample (or row) of input data gets its own value from the function call.  So each row is a result with it's own gradient.\n",
    "\n",
    "### But why is it done this way?\n",
    "\n",
    "Remember that each step in the computational graph retains the local gradient value from the forward pass.  The backward pass simply multiples each value in each step.  This follows the chain rule.\n",
    "\n",
    "But unlike our simple examples above, there are multiple variables and multiple rows.  So these local gradient values are not simple scalars, but a matrix of values of shape $n \\times m$, with **n** = number of rows and **m** = number of features.\n",
    "\n",
    "In our computational graph examples above, we always always started the back prop multiplication with 1, the derivative of the function with respect to itself.  \n",
    "\n",
    "This vector of ones is simply this same starting point, but we need ones for each row of data.\n",
    "\n",
    "PyTorch follows a process called the Jacobian Vector Product of JVP for back propagation.  The stored local gradient values is the Jacobian Matrix and the vector of ones is the Vector.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say the function result of a row is $y_1$.  Then the gradient with 3 features $[x_1, x_2, x_3]$ would look like this:\n",
    "\n",
    "$\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2}, \\frac{\\partial y_1}{\\partial x_3}$\n",
    "\n",
    "The gradient for the next row $y_2$ is:\n",
    "\n",
    "$\\frac{\\partial y_2}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2}, \\frac{\\partial y_2}{\\partial x_3}$\n",
    "\n",
    "Symbolically, the Jacobian looks like this for 3 features [$x_1, x_2, x_3$] with 3 rows of results [$y_1, y_2, y_3$]\n",
    "\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "  \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} \\\\ \n",
    "  \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_3}\\\\ \n",
    "  \\frac{\\partial y_3}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_3}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Since PyTorch doesn't do symbolic calculus, only the local gradients are stored.\n",
    "\n",
    "So the \"Jacobian\" might look something like:\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "  0.145 & 24.6 & 1.57 \\\\ \n",
    "  135 & 0.457 & 45.24\\\\ \n",
    " .001 & -75.4 & -22\n",
    "\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this Jacobian depends on where in the back propagation we are.  \n",
    "\n",
    "For most loss functions, we reduced all the features into a single loss value.  We also do some kind of average of all the samples or rows to produce a scalar value.  So the bigining of the backward process starts with the Jacobian as 1x1.\n",
    "\n",
    "The Jacobian shape will change as it goes back.  For example, a loss function that reduces features first, then reduces rows second would have the following shape changes, with n = number of rows and m = number of feature:\n",
    "- 1x1\n",
    "- nx1\n",
    "- nxm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first example $z = x^{3} + 2x^{2} + 8x$, we only had one function and one independent variable. so the Jacobian was simply:\n",
    "\n",
    "\n",
    "$\\begin{pmatrix}72\\end{pmatrix}$\n",
    "\n",
    "The first Jacobian Vector Product (JVP) for our one row one feature polynomial function would be:\n",
    "\n",
    "$\\begin{pmatrix}72\\end{pmatrix} \\cdot \\begin{pmatrix}1\\end{pmatrix} = \\begin{pmatrix}72\\end{pmatrix}$\n",
    "\n",
    "#### For functions that do NOT reduce the rows or variables, the vector of ones needs to match the shape of the output of the function\n",
    "\n",
    "The JVP might look like this:\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "  0.145 & 24.6 & 1.57 \\\\ \n",
    "  135 & 0.457 & 45.24 \\\\ \n",
    " .001 & -75.4 & -22\n",
    "\\end{pmatrix} \\odot \\begin{pmatrix}1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "  0.145 & 24.6 & 1.57 \\\\ \n",
    "  135 & 0.457 & 45.24 \\\\ \n",
    " .001 & -75.4 & -22\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to our Iris Example\n",
    "\n",
    "#### We have 2 major problems with our Iris example above!\n",
    "- The function is not a loss function comparing predicted to actual targets\n",
    "- The gradients are on the features\n",
    "\n",
    "Looking at the latter, knowing that sepal width is a strong influence does not help us.  Above, we would use these gradients to change the value of the inputs.  But we can't change the measured sepal width of an observation!!! \n",
    "\n",
    "We need to add weights to each observation.  Since we can change the weights but cannot change the observations, we will look at the gradients of the weights and make adjustments to these weights.\n",
    "\n",
    "#### Let's put in a real loss function first and add the weights later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      "sample of setosa: data = [5.0, 3.0, 1.6, 0.2]\n",
      "sample of versicolor: data = [6.6, 3.0, 4.4, 1.4]\n",
      "sample of virginica: data = [7.2, 3.2, 6.0, 1.8]\n",
      "\n",
      "Loss Function Results:\n",
      "sample of setosa: loss = 15.319999694824219\n",
      "sample of versicolor: loss = 0.0\n",
      "sample of virginica: loss = -33.839996337890625\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 2]) and output[0] has a shape of torch.Size([150]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-bbe2a78529b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mvector_of_ones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_of_ones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Gradients:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     27\u001b[0m                                    \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" and output[\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                                    \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"] has a shape of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                                    + str(out.shape) + \".\")\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 2]) and output[0] has a shape of torch.Size([150])."
     ]
    }
   ],
   "source": [
    "iris_samples_tensor = Variable(torch.FloatTensor(iris_samples), requires_grad=True)\n",
    "iris_targets_tensor = Variable(torch.FloatTensor(iris_targets), requires_grad=True)\n",
    "\n",
    "print('Input Data:')\n",
    "print(f'sample of setosa: data = {list(iris_samples_tensor[25].data.numpy())}')\n",
    "print(f'sample of versicolor: data = {list(iris_samples_tensor[75].data.numpy())}')\n",
    "print(f'sample of virginica: data = {list(iris_samples_tensor[125].data.numpy())}\\n') \n",
    "\n",
    "# Make same function we used above, but this is just an interim result\n",
    "iris_surface_areas = iris_surface_area_function(iris_samples_tensor)\n",
    "\n",
    "\"\"\"\n",
    "The loss funciton will:\n",
    " 1) Take in \"surface areas\"\n",
    " 2) Squash these areas using the sigmoid function\n",
    " 3) Use BCELoss (binary cross entropy) to return a loss value\n",
    "The cross entopy compares the sigmoid value to the actual binary target\n",
    "reduction='none' will retun a loss value for each row submitted\n",
    "\"\"\"\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "loss = loss_function(iris_surface_areas, iris_targets_tensor)\n",
    "\n",
    "print('Loss Function Results:')\n",
    "print(f'sample of setosa: loss = {loss[25].item()}')\n",
    "print(f'sample of versicolor: loss = {loss[75].item()}')\n",
    "print(f'sample of virginica: loss = {loss[125].item()}\\n')  \n",
    "\n",
    "vector_of_ones = torch.ones(row_count)\n",
    "loss.backward(vector_of_ones)\n",
    "\n",
    "print('Gradients:')\n",
    "grad_string = '[dl/d(sl), dl/d(sw), dl/d(pl), dl/d(pw)]' #with repect to sepal length/width petal length/width\n",
    "print(f'sample of setosa: {grad_string} = {list(iris_samples_tensor.grad.data[25].numpy())}') \n",
    "print(f'sample of versicolor: {grad_string} = {list(iris_samples_tensor.grad.data[75].numpy())}') \n",
    "print(f'sample of virginica: {grad_string} = {list(iris_samples_tensor.grad.data[125].numpy())}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, adding the loss function after our function call works the same\n",
    "\n",
    "Adding BCEWithLogitsLoss simply added more mathematical steps to the final result.  These additional steps add to the computation graph and are included in both the forward and backward passes.\n",
    "\n",
    "Remember, we are not trying to get a good model here, we just want to understand how the gradients work!\n",
    "\n",
    "### Now, let's add weights\n",
    "\n",
    "Since we can't change the observations, we can weigh the influence of each feature on this prediction.\n",
    "\n",
    "We will switch from getting the derivatives of the observations to getting the derivatives of the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_samples_tensor = Variable(torch.FloatTensor(iris_samples), requires_grad=True)\n",
    "iris_targets_tensor = Variable(torch.FloatTensor(iris_targets), requires_grad=True)\n",
    "initial_weights = np.random.uniform(low=0, high=1, size=iris_samples_tensor.shape)\n",
    "weights_tensor = Variable(torch.FloatTensor(initial_weights), requires_grad=True)\n",
    "\n",
    "print('Input Data:')\n",
    "print(f'sample of setosa: data = {list(iris_samples_tensor[25].data.numpy())}')\n",
    "print(f'sample of versicolor: data = {list(iris_samples_tensor[75].data.numpy())}')\n",
    "print(f'sample of virginica: data = {list(iris_samples_tensor[125].data.numpy())}\\n') \n",
    "\n",
    "def iris_weighted_surface_area_function(iris_samples_tensor, weights):\n",
    "    weighted_samples = torch.mul(weights, iris_samples_tensor)\n",
    "    wsl = weighted_samples[:,0]\n",
    "    wsw = weighted_samples[:,1]\n",
    "    wpl = weighted_samples[:,2]\n",
    "    wpw = weighted_samples[:,3]\n",
    "  \n",
    "    return wsl * wsw + wpl * wpw # Sort of area of sepal + area of petal\n",
    "\n",
    "# Make same function we used above, but this is just an interim result\n",
    "iris_surface_areas = iris_weighted_surface_area_function(iris_samples_tensor, weights_tensor)\n",
    "\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "loss = loss_function(iris_surface_areas, iris_targets_tensor)\n",
    "\n",
    "print('Loss Function Results:')\n",
    "print(f'sample of setosa: loss = {loss[25].item()}')\n",
    "print(f'sample of versicolor: loss = {loss[75].item()}')\n",
    "print(f'sample of virginica: loss = {loss[125].item()}\\n')  \n",
    "\n",
    "vector_of_ones = torch.ones(row_count)\n",
    "loss.backward(vector_of_ones)\n",
    "\n",
    "print('Weight Gradients:')\n",
    "grad_string = '[dl/d(w1), dl/d(w2), dl/d(w3), dl/d(w4)]' #with repect to sepal length/width petal length/width\n",
    "print(f'sample of setosa: {grad_string} = {list(weights_tensor.grad.data[25].numpy())}') \n",
    "print(f'sample of versicolor: {grad_string} = {list(weights_tensor.grad.data[75].numpy())}') \n",
    "print(f'sample of virginica: {grad_string} = {list(weights_tensor.grad.data[125].numpy())}\\n') \n",
    "\n",
    "print('Feature Gradients:')\n",
    "grad_string = '[dl/d(sl), dl/d(sw), dl/d(pl), dl/d(pw)]' #with repect to sepal length/width petal length/width\n",
    "print(f'sample of setosa: {grad_string} = {list(iris_samples_tensor.grad.data[25].numpy())}') \n",
    "print(f'sample of versicolor: {grad_string} = {list(iris_samples_tensor.grad.data[75].numpy())}') \n",
    "print(f'sample of virginica: {grad_string} = {list(iris_samples_tensor.grad.data[125].numpy())}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We not have gradients for both the features and the wights\n",
    "\n",
    "But we can only adjust the weight values since obviously the observation values are immutable.\n",
    "\n",
    "### Let's do a couple of manual iterations of this, but with the updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "def zeroIrisReset():\n",
    "    global iris_samples_tensor\n",
    "    global iris_targets_tensor\n",
    "    global weights_tensor\n",
    "    \n",
    "    try:\n",
    "        iris_samples_tensor.grad.data.zero_()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        iris_targets_tensor.grad.data.zero_() \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        weights_tensor = Variable(weights_tensor - torch.mul(learning_rate, weights_tensor.grad), requires_grad=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentIris(model_function):\n",
    "    global iris_samples_tensor\n",
    "    global iris_targets_tensor\n",
    "    global weights_tensor\n",
    "    \n",
    "    # We need to reset the gradients and adjust weights before the next run\n",
    "    zeroIrisReset()\n",
    "\n",
    "    model_scores = model_function(iris_samples_tensor, weights_tensor)\n",
    "\n",
    "    loss_function = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        \n",
    "    loss = loss_function(model_scores, iris_targets_tensor)\n",
    "\n",
    "    print('Loss Function Results:')\n",
    "    print(f'Average loss for Setosa: {loss[0:49].mean()}')\n",
    "    print(f'Average loss for Versicolor: {loss[50:99].mean()}')\n",
    "    print(f'Average loss for Virginica: {loss[100:149].mean()}\\n')  \n",
    "\n",
    "    vector_of_ones = torch.ones(row_count)\n",
    "    loss.backward(vector_of_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1, 11):\n",
    "    print(\"-\" * 30)\n",
    "    print(f'Iteration: {iteration}')\n",
    "    print(\"-\" * 30)\n",
    "    gradientDescentIris(iris_weighted_surface_area_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see the loss decreasing for 2 of the 3 targets\n",
    "\n",
    "This exercise was not to make a good model.  Our \"surface area\" function will be far less effective then the normal linear approach taken with most NNs.\n",
    "\n",
    "But hopefully, this should give a better feel for how automatic differentiation and back propagation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2*a\n",
    "\n",
    "b.retain_grad()   # Since b is non-leaf and it's grad will be destroyed otherwise.\n",
    "\n",
    "c = b.mean()\n",
    "\n",
    "c.backward()\n",
    "\n",
    "print(a.grad, b.grad)\n",
    "\n",
    "# Redo the experiment but with a hook that multiplies b's grad by 2. \n",
    "a = torch.ones(5)\n",
    "\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2*a\n",
    "\n",
    "b.retain_grad()\n",
    "\n",
    "b.register_hook(lambda x: print(x))  \n",
    "\n",
    "b.mean().backward() \n",
    "\n",
    "\n",
    "print(a.grad, b.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
